{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s_checklist_montecarlo_step03 [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=s_checklist_montecarlo_step03&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=ex-vue-3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t as tstu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from arpym.estimation.cov_2_corr import cov_2_corr\n",
    "from arpym.estimation.effective_num_scenarios import effective_num_scenarios\n",
    "from arpym.estimation.factor_analysis_paf import factor_analysis_paf\n",
    "from arpym.estimation.fit_locdisp_mlfp import fit_locdisp_mlfp\n",
    "from arpym.estimation.fit_locdisp_mlfp_difflength import fit_locdisp_mlfp_difflength\n",
    "from arpym.statistics.cop_marg_sep import cop_marg_sep\n",
    "from arpym.statistics.mvt_pdf import mvt_pdf\n",
    "from arpym.statistics.twist_prob_mom_match import twist_prob_mom_match\n",
    "from arpym.tools.colormap_fp import colormap_fp\n",
    "from arpym.tools.histogram_sp import histogram_sp\n",
    "from arpym.tools.logo import add_logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Input parameters](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for estimating marginal t distributions\n",
    "nu_min = 3  # lower bound for the degrees of freedom for t marginals\n",
    "nu_max = 100  # upper bound for the degrees of freedom for t marginals\n",
    "\n",
    "# parameters for estimating t copula\n",
    "nu_min_copula = 3  # lower bound for the degrees of freedom for t copula\n",
    "nu_max_copula = 5  # upper bound for the degrees of freedom for t copula\n",
    "\n",
    "# factor analysis\n",
    "k_ = 10  # number of factors for factor analysis\n",
    "\n",
    "# modeled invariant to plot\n",
    "i_plot = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 0](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step00): Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/databases/temporary-databases/'\n",
    "\n",
    "# Risk drivers identification\n",
    "db_riskdrivers_tools = pd.read_csv(path+'db_riskdrivers_tools.csv')\n",
    "n_stocks = int(db_riskdrivers_tools.n_stocks.dropna())\n",
    "n_bonds = int(db_riskdrivers_tools.n_bonds.dropna())\n",
    "d_implvol = int(db_riskdrivers_tools.d_implvol.dropna())\n",
    "i_bonds = n_bonds*4  # 4 Nelson-Siegel parameters x n_bonds\n",
    "\n",
    "# Quest for invariance\n",
    "db_invariants_series = pd.read_csv(path+'db_invariants_series.csv',\n",
    "                                   index_col=0, parse_dates=True)\n",
    "epsi = db_invariants_series.values\n",
    "t_, i_ = np.shape(epsi)\n",
    "dates = np.array(db_invariants_series.index)\n",
    "\n",
    "db_invariants_nextstep = pd.read_csv(path+'db_invariants_nextstep.csv')\n",
    "\n",
    "# market state indicator\n",
    "db_estimation_z = pd.read_csv(path+'db_estimation_z.csv',\n",
    "                                     index_col=0, parse_dates=True)\n",
    "\n",
    "# flexible probabilities\n",
    "db_estimation_flexprob = pd.read_csv(path+'db_estimation_flexprob.csv',\n",
    "                                     index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 1](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step01): Extract the flexible probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract market state for analysis dates\n",
    "z = db_estimation_z.z[dates].values\n",
    "z_star = z[-1]\n",
    "\n",
    "# extract flexible probabilities for analysis dates\n",
    "p = db_estimation_flexprob.p[dates].values\n",
    "p = p/np.sum(p)\n",
    "\n",
    "# effective number of scenarios\n",
    "ens = effective_num_scenarios(p)\n",
    "print('Effective number of scenarios is', int(round(ens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 2](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step02): Estimate the marginal distributions for stocks, S&P 500 and implied volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invariants to be modeled parametrically \n",
    "ind_parametric = np.arange(n_stocks+1+d_implvol,\n",
    "                         n_stocks+1+d_implvol+i_bonds)\n",
    "# invariants to be modeled nonparametrically\n",
    "ind_nonparametric = list(set(range(i_))-set(ind_parametric))\n",
    "db_estimation_nonparametric = {}  # contains the HFP marginals\n",
    "\n",
    "for i in ind_nonparametric:\n",
    "    # nonparametric estimation: stocks and S&P 500\n",
    "    if (db_invariants_nextstep.iloc[0, i] == 'GARCH(1,1)'):\n",
    "        p_tmp = twist_prob_mom_match(epsi[:, i], 0, 1, p)\n",
    "        db_estimation_nonparametric[i] = p_tmp\n",
    "\n",
    "    # nonparametric estimation: implied volatility\n",
    "    elif (db_invariants_nextstep.iloc[0, i] == 'Random walk'):\n",
    "        db_estimation_nonparametric[i] = p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 3](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step03): Estimate the marginal distributions for bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_estimation_parametric = {}  # contains the parameters of the t marginals\n",
    "\n",
    "# invariant series for bonds\n",
    "epsi_bonds = db_invariants_series.dropna().values\n",
    "# observations t_bonds for bonds\n",
    "t_bonds = epsi_bonds.shape[0]\n",
    "# rescale probabilities to sum to one over bond time frame\n",
    "p_bonds = p[-t_bonds:]/np.sum(p[-t_bonds:])\n",
    "# grid of values of degrees of freedom to test\n",
    "nu_vec = np.arange(nu_min, nu_max+1)\n",
    "j_ = len(nu_vec)\n",
    "\n",
    "for i in ind_parametric:\n",
    "    # parametric estimation (Student t): bonds\n",
    "    if (db_invariants_nextstep.iloc[0, i] == 'AR(1)'):\n",
    "        # time series has missing values\n",
    "        mu_nu = np.zeros(j_)\n",
    "        sig2_nu = np.zeros(j_)\n",
    "        llike_nu = np.zeros(j_)  # log-likelihood\n",
    "\n",
    "        # fit student t to marginals for a grid of values for nu\n",
    "        for j in range(j_):\n",
    "            nu = nu_vec[j]\n",
    "            # fit Student t model\n",
    "            mu_nu[j], sig2_nu[j] = fit_locdisp_mlfp(epsi_bonds[:, i],\n",
    "                                                    p=p_bonds, nu=nu)\n",
    "            # compute log-likelihood of Student t distribution\n",
    "            llike_nu[j] = np.sum(p_bonds*(np.log(np.sqrt(sig2_nu[j])) +\n",
    "                                 tstu.logpdf(epsi_bonds[:, i], nu, mu_nu[j],\n",
    "                                             np.sqrt(sig2_nu[j]))))\n",
    "\n",
    "        # choose nu that gives the highest log-likelihood\n",
    "        j_max = np.argsort(llike_nu)[-1]\n",
    "        db_estimation_parametric[i] = {'invariant': i,\n",
    "                                       'nu': nu_vec[j_max],\n",
    "                                       'mu': mu_nu[j_max],\n",
    "                                       'sig2': sig2_nu[j_max]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 4](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step04): Extract grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsi_t = np.zeros((t_bonds, i_))\n",
    "u = np.zeros((t_, i_))\n",
    "\n",
    "# compute the (realizations of the) grades U\n",
    "for i in range(i_):\n",
    "    # nonparametric estimation: stocks, S&P 500 and implied volatility\n",
    "    if i in ind_nonparametric:\n",
    "        u[:, i], _, _ = cop_marg_sep(epsi[:, i],\n",
    "                                     db_estimation_nonparametric[i])\n",
    "\n",
    "    # parametric estimation (Student t): bonds\n",
    "    elif i in ind_parametric:\n",
    "        epsi_t[:, i] = (epsi_bonds[:, i] - db_estimation_parametric[i]['mu']) \\\n",
    "                        / np.sqrt(db_estimation_parametric[i]['sig2'])\n",
    "        u[-t_bonds:, i] = tstu.cdf(epsi_t[:, i],\n",
    "                                   db_estimation_parametric[i]['nu'])\n",
    "        # values must be < 1 for Student t\n",
    "        u[-t_bonds:, i] = np.minimum(u[-t_bonds:, i], 0.99999999)\n",
    "        u[:-t_bonds, i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 5](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step05): Estimate the copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flexible probabilities for the Student t copula estimation via MLFP\n",
    "p_copula = p\n",
    "p_copula_bonds = p[-t_bonds:]\n",
    "\n",
    "# grid for the degrees of freedom parameter\n",
    "nu_vec_cop = np.arange(nu_min_copula, nu_max_copula+1)\n",
    "l_ = len(nu_vec_cop)\n",
    "\n",
    "# initialize variables\n",
    "rho2_copula_vec = np.zeros((i_, i_, l_))\n",
    "llike_nu = np.zeros(l_)\n",
    "epsi_tilde = np.zeros((t_, i_, l_))\n",
    "\n",
    "db_estimation_copula = {}\n",
    "\n",
    "for l in range(l_):\n",
    "    # calculate standardized invariants\n",
    "    for i in range(i_):\n",
    "        epsi_tilde[:, i, l] = tstu.ppf(u[:, i], nu_vec_cop[l])\n",
    "\n",
    "    # estimate copula parameters with maximum likelihood\n",
    "    _, sig2 =\\\n",
    "        fit_locdisp_mlfp_difflength(epsi_tilde[:, :, l],\n",
    "                                    p=p_copula,\n",
    "                                    nu=nu_vec_cop[l],\n",
    "                                    threshold=10**-3,\n",
    "                                    maxiter=1000)\n",
    "\n",
    "    # shrinkage: factor analysis\n",
    "    beta, delta2 = factor_analysis_paf(sig2, k_)\n",
    "    sig2_fa = beta@beta.T + np.diag(delta2)\n",
    "    \n",
    "    # compute correlation matrix\n",
    "    rho2_copula_vec[:, :, l], _ = cov_2_corr(sig2_fa)\n",
    "\n",
    "    # compute log-likelihood at times with no missing values\n",
    "    llike_nu[l] = np.sum(p_copula_bonds *\n",
    "                         np.log(mvt_pdf(epsi_bonds, np.zeros(i_),\n",
    "                                        rho2_copula_vec[:, :, l],\n",
    "                                        nu_vec_cop[l])))\n",
    "\n",
    "# choose nu that gives the highest log-likelihood\n",
    "l_nu = np.argsort(llike_nu)[-1]\n",
    "db_estimation_copula = {'nu': np.int(nu_vec_cop[l_nu]),\n",
    "                        'rho2': rho2_copula_vec[:, :, l_nu]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 6](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step06): Estimate the distribution of the credit structural invariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_credit = np.array(\n",
    "    [np.where(db_invariants_series.columns == 'stock GE_log_value')[0][0],\n",
    "    np.where(db_invariants_series.columns == 'stock JPM_log_value')[0][0]]\n",
    ")\n",
    "\n",
    "# extract degrees of freedom\n",
    "nu_credit = np.int(nu_vec_cop[l_nu])\n",
    "\n",
    "# extract correlation\n",
    "rho2_credit = rho2_copula_vec[:, ind_credit, l_nu][ind_credit, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 7](https://www.arpm.co/lab/redirect.php?permalink=s_checklist_montecarlo_step03-implementation-step07): Save databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametric estimation\n",
    "out = pd.DataFrame(db_estimation_parametric,\n",
    "                   columns=[db_estimation_parametric[i]['invariant']\n",
    "                            for i in ind_parametric])\n",
    "out.to_csv(path+'db_estimation_parametric.csv')\n",
    "del out\n",
    "\n",
    "# nonparametric estimation\n",
    "out = pd.DataFrame(db_estimation_nonparametric, columns=ind_nonparametric)\n",
    "out.to_csv(path+'db_estimation_nonparametric.csv',\n",
    "           index=False)\n",
    "del out\n",
    "\n",
    "# copula degrees of freedom and correlation matrix\n",
    "out = pd.DataFrame({'nu': pd.Series(db_estimation_copula['nu']),\n",
    "                    'rho2':\n",
    "                        pd.Series(db_estimation_copula['rho2'].reshape(-1))})\n",
    "out.to_csv(path+'db_estimation_copula.csv',\n",
    "           index=None)\n",
    "del out\n",
    "\n",
    "# credit copula degrees of freedom and correlation matrix\n",
    "out = pd.DataFrame({'nu_credit': pd.Series(nu_credit),\n",
    "                    'rho2_credit':\n",
    "                        pd.Series(rho2_credit.reshape(-1))})\n",
    "out.to_csv(path+'db_estimation_credit_copula.csv',\n",
    "           index=None)\n",
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('arpm')\n",
    "\n",
    "# VIX\n",
    "date_tick = np.arange(0, t_-1, 200)\n",
    "fig1 = plt.figure(figsize=(1280.0/72.0, 720.0/72.0), dpi=72.0)\n",
    "ax1 = fig1.add_subplot(311)\n",
    "plt.plot(dates, z, color=[0, 0, 0], lw=1.15)\n",
    "plt.title('Market state', fontweight='bold', fontsize=20)\n",
    "plt.xticks(dates[date_tick], fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([min(dates), max(dates)])\n",
    "plt.plot(dates, z_star*np.ones(len(dates)), color='red', lw=1.25)\n",
    "plt.legend(['Market state', 'Target value'], fontsize=17)\n",
    "\n",
    "# select appropriate probabilities for plots\n",
    "if i_plot-1 in ind_parametric:\n",
    "    # bonds\n",
    "    add_pad = dates.shape[0] - p_bonds.shape[0]\n",
    "    p_plot = np.pad(p_bonds, (add_pad, 0), 'constant', constant_values=0)\n",
    "else:\n",
    "    # stocks, S&P 500 and implied volatility (options)\n",
    "    p_plot = db_estimation_nonparametric[i_plot-1]\n",
    "\n",
    "# flexible probabilities\n",
    "myFmt = mdates.DateFormatter('%d-%b-%Y')\n",
    "ax2 = fig1.add_subplot(312)\n",
    "plt.bar(dates, p_plot.flatten(), color='gray')\n",
    "plt.xlim([min(dates), max(dates)])\n",
    "plt.title('Time and state conditioning flexible probabilities',\n",
    "          fontweight='bold', fontsize=20)\n",
    "plt.xticks(dates[date_tick], fontsize=14)\n",
    "plt.yticks([], fontsize=14)\n",
    "plt.xlim([min(dates), max(dates)])\n",
    "ax2.xaxis.set_major_formatter(myFmt)\n",
    "\n",
    "# flexible probabilities scatter for invariant i_plot\n",
    "ax3 = fig1.add_subplot(313)\n",
    "grey_range = np.r_[np.arange(0, 0.6 + 0.01, 0.01), .85]\n",
    "[color_map, p_colors] = colormap_fp(p_plot, np.min(p_plot), np.max(p_plot),\n",
    "                                    grey_range, 0, 10, [10, 0])\n",
    "p_colors = p_colors.T\n",
    "\n",
    "plt.xticks(dates[date_tick], fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([min(dates), max(dates)])\n",
    "plt.scatter(dates, epsi[:, i_plot-1], s=30, c=p_colors, marker='.',\n",
    "            cmap=color_map)\n",
    "plt.title(db_invariants_series.columns[i_plot-1] + ' observation weighting',\n",
    "          fontweight='bold', fontsize=20)\n",
    "ax3.xaxis.set_major_formatter(myFmt)\n",
    "add_logo(fig1, location=1, set_fig_size=False)\n",
    "fig1.tight_layout()\n",
    "\n",
    "# marginal distributions\n",
    "\n",
    "n_bins = 10 * np.log(t_)\n",
    "\n",
    "hfp = plt.figure(figsize=(1280.0/72.0, 720.0/72.0), dpi=72.0)\n",
    "ax = hfp.add_subplot(111)\n",
    "\n",
    "if i_plot-1 in ind_parametric:\n",
    "    # HFP histogram\n",
    "    f_eps, x_eps = histogram_sp(epsi_bonds[:, i_plot-1],\n",
    "                                p=p_bonds, k_=n_bins)\n",
    "    bar_width = x_eps[1] - x_eps[0]\n",
    "    plt.bar(x_eps, f_eps.flatten(), width=bar_width, fc=[0.7, 0.7, 0.7],\n",
    "            edgecolor=[0.5, 0.5, 0.5])\n",
    "\n",
    "    # Student t fit\n",
    "    plt.plot(x_eps, np.squeeze(\n",
    "             tstu.pdf(x_eps, db_estimation_parametric[i_plot-1]['nu'],\n",
    "                      db_estimation_parametric[i_plot-1]['mu'],\n",
    "                      np.sqrt(db_estimation_parametric[i_plot-1]['sig2']))))\n",
    "\n",
    "else:\n",
    "    # HFP histogram\n",
    "    f_eps, x_eps = histogram_sp(epsi[:, i_plot-1],\n",
    "                                p=db_estimation_nonparametric[i_plot-1],\n",
    "                                k_=n_bins)\n",
    "    bar_width = x_eps[1] - x_eps[0]\n",
    "    plt.bar(x_eps, f_eps.flatten(), width=bar_width, fc=[0.7, 0.7, 0.7],\n",
    "            edgecolor=[0.5, 0.5, 0.5])\n",
    "\n",
    "plt.title(db_invariants_series.columns[i_plot-1] + ' invariant distribution',\n",
    "         fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Invariant', fontsize=17)\n",
    "add_logo(hfp, location=1, set_fig_size=False)\n",
    "hfp.tight_layout()\n",
    "\n",
    "# copula correlation matrix\n",
    "fig3 = plt.figure(figsize=(1280.0/72.0, 720.0/72.0), dpi=72.0)\n",
    "\n",
    "# create colormap\n",
    "colors = ['#FF9900', '#FFFFFF', '#3C9591']\n",
    "arpm_cmap = LinearSegmentedColormap.from_list('arpm_cmap', colors)\n",
    "\n",
    "plt.imshow(db_estimation_copula['rho2'], cmap=arpm_cmap, vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Estimated correlation matrix', fontweight='bold', fontsize=20)\n",
    "add_logo(fig3, size_frac_x=0.8, location=8, alpha=0.8, set_fig_size=False)\n",
    "fig3.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
