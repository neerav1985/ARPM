{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_CopulaMarginalDistribution [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=S_CopulaMarginalDistribution&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=ExerCopulaMargDist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "sys.path.append(path.abspath('../../functions-legacy'))\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange, array, ones, zeros, sort, argsort, diff, diag, abs, log, exp, sqrt, r_\n",
    "from numpy import sum as npsum, min as npmin, max as npmax\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, plot, xlim, scatter, ylabel, \\\n",
    "    xlabel, title, xticks\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from CONFIG import GLOBAL_DB, TEMPORARY_DB\n",
    "from ARPM_utils import save_plot, struct_to_dict, date_mtop\n",
    "from intersect_matlab import intersect\n",
    "from ConditionalFP import ConditionalFP\n",
    "from BootstrapNelSieg import BootstrapNelSieg\n",
    "from Tscenarios import Tscenarios\n",
    "from FactorAnalysis import FactorAnalysis\n",
    "from MaxLikelihoodFPLocDispT import MaxLikelihoodFPLocDispT\n",
    "from CopMargComb import CopMargComb\n",
    "\n",
    "# parameters\n",
    "par_start = namedtuple('par','theta1 theta2 theta3 theta4_squared')\n",
    "par_start.theta1 = 0.05\n",
    "par_start.theta2 = 0.05\n",
    "par_start.theta3 = 0.05\n",
    "par_start.theta4_squared = 0.05  #\n",
    "tau = array([0.0833,1,2,3,4,5,6,7,8,9,10,15,20,25,30])  # Starting values and time to maturities for NS parameters time series extraction\n",
    "tau_HL = 80  # Half life parameter (days)\n",
    "nu = 4  # degrees of freedom of the t copula we want to fit\n",
    "nu_vec = arange(2,31)\n",
    "nu_ = len(nu_vec)\n",
    "j_ = 2000  # number of scenarios\n",
    "k_ = 1  # factors for correlation shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_Stocks'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_Stocks'), squeeze_me=True)\n",
    "\n",
    "SPX = struct_to_dict(db['SPX'])\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_CorporateBonds'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_CorporateBonds'), squeeze_me=True)\n",
    "\n",
    "GE = struct_to_dict(db['GE'])\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_VIX'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_VIX'), squeeze_me=True)\n",
    "\n",
    "VIX = struct_to_dict(db['VIX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time series of daily S&P500 index's returns and extract the daily time series of VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S&P 500\n",
    "ret_SP500 = diff(log(SPX.Price_close))\n",
    "DateSP = SPX.Date[1:]\n",
    "\n",
    "# Conditioning variable: VIX\n",
    "DateVIX = VIX.Date\n",
    "vix = VIX.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time series of daily increments of the Nielson-Siegel parameters of the spot yield curve for the GE bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bond schedule\n",
    "b_sched_GE = zeros((max(GE.Coupons.shape[0],GE.Expiry_Date.shape[0]),2))\n",
    "b_sched_GE[:, 0] = GE.Coupons/100\n",
    "b_sched_GE[:, 1] = GE.Expiry_Date\n",
    "\n",
    "# prices\n",
    "b_GE = GE.Dirty_Prices/100\n",
    "\n",
    "# NS parameters' daily increments time series\n",
    "\n",
    "t_ = len(GE.Date)\n",
    "thetaGE = zeros((4, t_))\n",
    "thetaGE[0], thetaGE[1], thetaGE[2], thetaGE[3], *_ = BootstrapNelSieg(GE.Date, b_GE, b_sched_GE, tau, par_start)\n",
    "DateGE = GE.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the observations in the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, idx_sp, idx_GE = intersect(DateSP, DateGE)\n",
    "ret_SP500 = ret_SP500[idx_sp]\n",
    "thetaGE = thetaGE[:, idx_GE]\n",
    "dates, I_sp_ge, I_vix = intersect(date, DateVIX)\n",
    "ret_SP500 = ret_SP500[I_sp_ge]\n",
    "thetaGE = thetaGE[:, I_sp_ge]\n",
    "vix = vix[I_vix]\n",
    "\n",
    "epsi = r_[ret_SP500[np.newaxis,...], thetaGE]\n",
    "i_, t_ = epsi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Flexible Probabilities conditioned on VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior\n",
    "lam = log(2) / tau_HL\n",
    "prior = exp(-lam*abs(arange(t_, 1 + -1, -1))).reshape(1,-1)\n",
    "prior = prior / npsum(prior)\n",
    "\n",
    "# conditioner\n",
    "conditioner = namedtuple('conditioner', ['Series', 'TargetValue', 'Leeway'])\n",
    "conditioner.Series = vix.reshape(1,-1)\n",
    "conditioner.TargetValue = array([[vix[-1]]])\n",
    "conditioner.Leeway = 0.3\n",
    "# Flexible Probabilities\n",
    "p = ConditionalFP(conditioner, prior)\n",
    "# ## Fit the t copula\n",
    "# ## estimate marginal distributions by fitting a Student t distribution via\n",
    "# ## MLFP and recover the invariants' grades\n",
    "u = zeros((i_, t_))\n",
    "epsi= sort(epsi, 1)  # We sort scenario in ascending order (in order to apply CopMargComb later)\n",
    "for i in range(i_):\n",
    "    mu_nu = zeros(nu_)\n",
    "    sig2_nu = zeros(nu_)\n",
    "    like_nu = zeros(nu_)\n",
    "    for k in range(nu_):\n",
    "        nu_k = nu_vec[k]\n",
    "        mu_nu[k], sig2_nu[k],_ = MaxLikelihoodFPLocDispT(epsi[[i],:], p, nu_k, 10 ** -6, 1)\n",
    "        epsi_t = (epsi[[i], :] - mu_nu[k]) / sqrt(sig2_nu[k])\n",
    "        like_nu[k] = npsum(p * log(t.pdf(epsi_t, nu_k) / sqrt(sig2_nu[k])))  # likelihood\n",
    "        j_nu = argsort(like_nu)[::-1]\n",
    "        # take as estimates the parameters giving rise to the highest likelihood\n",
    "    nu_marg = max(nu_vec[j_nu[0]], 10)\n",
    "    mu_marg = mu_nu[j_nu[0]]\n",
    "    sig2_marg = sig2_nu[j_nu[0]]\n",
    "    u[i, :] = t.cdf((epsi[i, :] - mu_marg) / sqrt(sig2_marg), nu_marg)\n",
    "# Map the grades into standard Student t realizations\n",
    "epsi_tilde = zeros((i_, t_))\n",
    "for i in range(i_):\n",
    "    epsi_tilde[i,:] = t.ppf(u[i, :], nu)\n",
    "\n",
    "# fit the ellipsoid via MLFP\n",
    "\n",
    "mu, sigma2,_ = MaxLikelihoodFPLocDispT(epsi_tilde, p, nu, 10 ** -6, 1)\n",
    "\n",
    "# Shrink the correlation matrix toward a low-rank-diagonal structure\n",
    "rho2 = np.diagflat(diag(sigma2) ** (-1 / 2))@sigma2@np.diagflat(diag(sigma2) ** (-1 / 2))\n",
    "rho2,*_ = FactorAnalysis(rho2, array([[0]]), k_)\n",
    "rho2 = np.real(rho2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate scenarios from the estimated t copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optionT = namedtuple('option', 'dim_red stoc_rep')\n",
    "optionT.dim_red = 0\n",
    "optionT.stoc_rep = 0\n",
    "tcop_scen = Tscenarios(nu, zeros((i_, 1)), rho2, j_, optionT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the copula-marginal distribution scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_MC = t.cdf(tcop_scen, nu)\n",
    "epsi_MC = CopMargComb(epsi, u, grades_MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "# scatter plot\n",
    "scatter(100*epsi_MC[0], epsi_MC[1], s=10, c=[0.6, 0.6, 0.6], marker='*')\n",
    "title('COPULA-MARGINAL Distribution')\n",
    "xlabel('S&P 500 daily return (%)')\n",
    "ylabel('NS first parameter')\n",
    "plt.axis([100*npmin(epsi_MC[0])- 0.01, 100*npmax(epsi_MC[0]) + 0.01, npmin(epsi_MC[1]) - 0.01, npmax(epsi_MC[1])+0.01])\n",
    "# vix plot\n",
    "date_xtick = arange(99, len(vix), 380)\n",
    "dates_dt = array([date_mtop(i) for i in dates])\n",
    "xticklab = dates_dt[date_xtick];\n",
    "myFmt = mdates.DateFormatter('%d-%b-%Y');\n",
    "# save_plot(ax=plt.gca(), extension='png', scriptname=os.path.basename('.')[:-3], count=plt.get_fignums()[-1])\n",
    "\n",
    "figure()\n",
    "ax1 = plt.gca()\n",
    "plt.bar(dates_dt, p[0], width=dates_dt[1].toordinal()-dates_dt[0].toordinal(),color= 'grey')\n",
    "ax1.xaxis.set_major_formatter(myFmt)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(dates_dt, vix, color=[0, 0, 0.6],lw=1)\n",
    "ax2.plot(dates_dt, conditioner.TargetValue[0]*ones(t_),color='r',linestyle='--',lw=1)\n",
    "ax1.set_xlim([min(dates_dt), max(dates_dt)])\n",
    "ax1.set_xticks(xticklab)\n",
    "ax1.set_ylim([0, 1.1*npmax(p)])\n",
    "ax1.set_yticks([])\n",
    "ax2.set_yticks(arange(20,100,20))\n",
    "ax2.set_ylim([npmin(vix), 1.1*npmax(vix)])\n",
    "ax2.set_ylabel('VIX',color=[0, 0, 0.6])\n",
    "title('Flexible Probabilities');\n",
    "# save_plot(ax=plt.gca(), extension='png', scriptname=os.path.basename('.')[:-3], count=plt.get_fignums()[-1])\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
