{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s_default_probabilities [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=s_default_probabilities&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=eb-supervised-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, \\\n",
    "                                                            QuantileTransformer\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "from arpym.tools.logo import add_logo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Input parameters](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2  # proportion of the test set\n",
    "n_sample = 10000  # num. of samples in the database; set =30000 to catch it all\n",
    "pol_degree = 2  # degrees in polynomial features\n",
    "lambda_lasso = 0.05  # lasso parameter\n",
    "max_depth_tree = 10  # maximum depth of decision tree classifier\n",
    "cross_val = 1  # set \"1\" to do cross-validation (computational time increases)\n",
    "k_ = 5  # parameter of Stratified K-Folds cross-validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 0](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step00): Import data and pre-process database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "path = '~/databases/global-databases/credit/' + \\\n",
    "    'db_default_data_creditcardsclients/'\n",
    "df = pd.read_csv(path+'db_default_data_creditcardsclients.csv')\n",
    "df = df.iloc[:, 1:df.shape[1]]  # exlude ID\n",
    "\n",
    "# Sort database so that the categorical features are at the beginning\n",
    "\n",
    "# indexes of the categorical features\n",
    "ind_cat = np.r_[np.arange(1, 4), np.arange(5, 11)]\n",
    "n_cat = len(ind_cat)  # number of categorical features\n",
    "# indexes of the continuous features\n",
    "ind_cont = np.r_[np.array([0, 4]), np.arange(11, df.shape[1])]\n",
    "n_cont = len(ind_cont)  # number of categorical features\n",
    "df = df.iloc[:n_sample, np.r_[ind_cat, ind_cont]]\n",
    "\n",
    "# Outputs and features\n",
    "z = np.array(df.iloc[:, :-1])  # features\n",
    "x = np.array(df.iloc[:, -1])  # labels\n",
    "\n",
    "# Standardize continuous features\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "z_cont = quantile_transformer.fit_transform(z[:, -n_cont:])\n",
    "\n",
    "# Transform categorical features via one-hot encoding\n",
    "# shift up, because the OneHotEncoder takes only positive inputs\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "z_cat = enc.fit_transform(np.abs(np.min(z[:, :n_cat], axis=0)) +\n",
    "                          z[:, :n_cat]).toarray()\n",
    "\n",
    "n_enc = z_cat.shape[1]  # number of encoded categorical features\n",
    "\n",
    "z = np.concatenate((z_cat, z_cont), axis=1)\n",
    "\n",
    "# Define test set and estimation set\n",
    "z_estimation, z_test, x_estimation, x_test = train_test_split(z, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 1](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step01): Logistic regression on continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set C = +infinity to have 0 Lasso parameter\n",
    "lg = LogisticRegression(penalty='l2', C=10**9, solver='lbfgs')\n",
    "lg = lg.fit(z_estimation[:, -n_cont:], x_estimation)  # fit the model\n",
    "p_z_lg = lg.predict_proba(z_test[:, -n_cont:])[:, 1]  # predict the probs\n",
    "cm_lg = confusion_matrix(x_test, lg.predict(z_test[:, -n_cont:]))  # conf. mat.\n",
    "er_lg = -np.sum(np.log(p_z_lg))/len(p_z_lg)  # error\n",
    "print('Logistic error: %1.4f' % er_lg)\n",
    "# conditional scores\n",
    "s_0_lg = logit(lg.predict_proba(z_test[:, -n_cont:])[\n",
    "                    np.where(x_test == 0)[0], 1])\n",
    "s_1_lg = logit(lg.predict_proba(z_test[:, -n_cont:])[\n",
    "                    np.where(x_test == 1)[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 2](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step02): Add interactions to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interactions\n",
    "poly = PolynomialFeatures(degree=pol_degree)\n",
    "z_estimation_inter = poly.fit_transform(z_estimation[:, -n_cont:])\n",
    "z_test_inter = poly.fit_transform(z_test[:, -n_cont:])\n",
    "\n",
    "# Set C = +infinity to have 0 Lasso parameter\n",
    "lg_inter = LogisticRegression(penalty='l2', C=10**9, solver='lbfgs')\n",
    "lg_inter = lg_inter.fit(z_estimation_inter, x_estimation)  # fit the model\n",
    "p_z_inter = lg_inter.predict_proba(z_test_inter)[:, 1]  # pred. the probs.\n",
    "cm_inter = confusion_matrix(x_test, lg_inter.predict(z_test_inter))\n",
    "er_inter = -np.sum(np.log(p_z_inter))/len(p_z_inter)  # error\n",
    "print('Logistic with interactions error: %1.4f' % er_inter)\n",
    "# conditional scores\n",
    "s_0_inter = logit(lg_inter.predict_proba(z_test_inter)[\n",
    "                                                  np.where(x_test == 0)[0], 1])\n",
    "s_1_inter = logit(lg_inter.predict_proba(z_test_inter)[\n",
    "                                                  np.where(x_test == 1)[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 3](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step03): Add encoded categorical features to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_enc_estimation = np.concatenate((z_estimation[:, :n_enc],\n",
    "                                   z_estimation_inter), axis=1)\n",
    "z_enc_test = np.concatenate((z_test[:, :n_enc], z_test_inter), axis=1)\n",
    "\n",
    "# Set C = +infinity to have 0 Lasso parameter\n",
    "lg_enc = LogisticRegression(penalty='l2', C=10**9, solver='lbfgs')\n",
    "lg_enc = lg_enc.fit(z_enc_estimation, x_estimation)  # fit the model\n",
    "p_z_enc = lg_enc.predict_proba(z_enc_test)[:, 1]  # pred. the probs.\n",
    "cm_enc = confusion_matrix(x_test, lg_enc.predict(z_enc_test))\n",
    "er_enc = -np.sum(np.log(p_z_enc))/len(p_z_enc)  # error\n",
    "print('Logistic with interactions and categorical error: %1.4f' % er_enc)\n",
    "# conditional scores\n",
    "s_0_enc = logit(lg_enc.predict_proba(z_enc_test)[np.where(x_test == 0)[0], 1])\n",
    "s_1_enc = logit(lg_enc.predict_proba(z_enc_test)[np.where(x_test == 1)[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 4](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step04): Add lasso regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_lasso = LogisticRegression(penalty='l1', C=1/lambda_lasso, solver='liblinear')\n",
    "lg_lasso = lg_lasso.fit(z_enc_estimation, x_estimation)  # fit the model\n",
    "p_z_lasso = lg_lasso.predict_proba(z_enc_test)[:, 1]  # predict the probs.\n",
    "cm_lasso = confusion_matrix(x_test, lg_lasso.predict(z_enc_test))  # conf. mat.\n",
    "er_lasso = -np.sum(np.log(p_z_lasso))/len(p_z_lasso)  # error\n",
    "print('Logistic with lasso error: %1.4f' % er_lasso)\n",
    "# conditional scores\n",
    "s_0_lasso = logit(lg_lasso.predict_proba(z_enc_test)[\n",
    "                    np.where(x_test == 0)[0], 1])\n",
    "s_1_lasso = logit(lg_lasso.predict_proba(z_enc_test)[\n",
    "                    np.where(x_test == 1)[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 5](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step05): CART classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(max_depth=max_depth_tree)  # def. method\n",
    "tree_clf = tree_clf.fit(z_enc_estimation, x_estimation)  # fit the model\n",
    "p_z_tree = tree_clf.predict_proba(z_enc_test)[:, 1]  # predict the scores\n",
    "cm_tree = confusion_matrix(x_test, tree_clf.predict(z_enc_test))  # conf. mat.\n",
    "er_tree = (cm_tree[0, 1]/np.sum(x_test == 0) +\n",
    "           cm_tree[1, 0]/np.sum(x_test == 1))  # error\n",
    "print('CART classifier error: %1.4f' % er_tree)\n",
    "# conditional scores\n",
    "eps = 10**-5  # set threshold to avoid numerical noise in the logit function\n",
    "p_0_tree = tree_clf.predict_proba(z_enc_test)[np.where(x_test == 0)[0], 1]\n",
    "p_0_tree[p_0_tree < eps] = eps\n",
    "p_0_tree[p_0_tree > 1-eps] = 1-eps\n",
    "p_1_tree = tree_clf.predict_proba(z_enc_test)[np.where(x_test == 1)[0], 1]\n",
    "p_1_tree[p_1_tree < eps] = eps\n",
    "p_1_tree[p_1_tree > 1-eps] = 1-eps\n",
    "s_0_tree = logit(p_0_tree)\n",
    "s_1_tree = logit(p_1_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 6](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step06): Add gradient boosting to CART classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_clf = GradientBoostingClassifier(max_depth=max_depth_tree)  # method\n",
    "boost_clf = boost_clf.fit(z_enc_estimation, x_estimation)  # fit the model\n",
    "p_z_boost = boost_clf.predict_proba(z_enc_test)[:, 1]  # predict the probs.\n",
    "cm_boost = confusion_matrix(x_test, boost_clf.predict(z_enc_test))  # conf. mat\n",
    "er_boost = (cm_boost[0, 1]/np.sum(x_test == 0) +\n",
    "            cm_boost[1, 0]/np.sum(x_test == 1))  # error\n",
    "print('CART classifier with gradient boosting error: %1.4f' % er_boost)\n",
    "# conditional scores\n",
    "s_0_boost = logit(boost_clf.predict_proba(z_enc_test)[\n",
    "                np.where(x_test == 0)[0], 1])\n",
    "s_1_boost = logit(boost_clf.predict_proba(z_enc_test)[\n",
    "                np.where(x_test == 1)[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 7](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step07): Compute fpr, tpr and AUC on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Logistic\n",
    "fpr_lg, tpr_lg, _ = roc_curve(x_test, p_z_lg)\n",
    "auc_lg = auc(fpr_lg, tpr_lg)\n",
    "print('Logistic AUC: %1.3f' % auc_lg)\n",
    "\n",
    "# 2) Logistic with interactions\n",
    "fpr_inter, tpr_inter, _ = roc_curve(x_test, p_z_inter)\n",
    "auc_inter = auc(fpr_inter, tpr_inter)\n",
    "print('Logistic with interactions AUC: %1.3f' % auc_inter)\n",
    "\n",
    "# 3) Logistic with interactions and encoded categorical features\n",
    "fpr_enc, tpr_enc, _ = roc_curve(x_test, p_z_enc)\n",
    "auc_enc = auc(fpr_enc, tpr_enc)\n",
    "print('Logistic with interactions and categorical AUC: %1.3f' % auc_enc)\n",
    "\n",
    "# 4) Logistic lasso with interactions and encoded categorical features\n",
    "fpr_lasso, tpr_lasso, _ = roc_curve(x_test, p_z_lasso)\n",
    "auc_lasso = auc(fpr_lasso, tpr_lasso)\n",
    "print('Logistic with lasso AUC: %1.3f' % auc_lasso)\n",
    "\n",
    "# 5) CART classifier\n",
    "fpr_tree, tpr_tree, _ = roc_curve(x_test, p_z_tree)\n",
    "auc_tree = auc(fpr_tree, tpr_tree)\n",
    "print('CART classifier AUC: %1.3f' % auc_tree)\n",
    "\n",
    "# 6) Gradient boosting classifier\n",
    "fpr_boost, tpr_boost, _ = roc_curve(x_test, p_z_boost)\n",
    "auc_boost = auc(fpr_boost, tpr_boost)\n",
    "print('Gradient boosting classifier AUC: %1.3f' % auc_boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 8](https://www.arpm.co/lab/redirect.php?permalink=s_default_probabilities-implementation-step08): Choose best probabilistic and point predictors via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cross_val == 1:\n",
    "    # Split the estimation set into training and validation sets for k-fold\n",
    "    # cross-validation\n",
    "    k_fold = StratifiedKFold(n_splits=k_)\n",
    "    z_train = []\n",
    "    z_train_inter = []\n",
    "    z_train_enc = []\n",
    "    x_train = []\n",
    "    z_val = []\n",
    "    z_val_inter = []\n",
    "    z_val_enc = []\n",
    "    x_val = []\n",
    "    for train, val in k_fold.split(z_estimation, x_estimation):\n",
    "        z_train.append(z_estimation[train])\n",
    "        x_train.append(x_estimation[train])\n",
    "        z_val.append(z_estimation[val])\n",
    "        x_val.append(x_estimation[val])\n",
    "    for train, val in k_fold.split(z_estimation_inter, x_estimation):\n",
    "        z_train_inter.append(z_estimation_inter[train])\n",
    "        z_val_inter.append(z_estimation_inter[val])\n",
    "    for train, val in k_fold.split(z_enc_estimation, x_estimation):\n",
    "        z_train_enc.append(z_enc_estimation[train])\n",
    "        z_val_enc.append(z_enc_estimation[val])\n",
    "    # Probabilistic\n",
    "    cv_er_lg = []\n",
    "    cv_er_lasso = []\n",
    "    cv_er_inter = []\n",
    "    cv_er_enc = []\n",
    "    for k in range(k_):\n",
    "        # Logistic\n",
    "        p_cv_lg = lg.fit(z_train[k], x_train[k]).predict_proba(z_val[k])\n",
    "        cv_er_lg.append(-np.sum(np.log(p_cv_lg))/len(p_cv_lg))\n",
    "\n",
    "        # Lasso\n",
    "        p_cv_lasso = lg_lasso.fit(z_train[k],\n",
    "                                  x_train[k]).predict_proba(z_val[k])\n",
    "        cv_er_lasso.append(-np.sum(np.log(p_cv_lasso))/len(p_cv_lasso))\n",
    "\n",
    "        # Interactions\n",
    "        p_cv_inter = lg_inter.fit(z_train_inter[k],\n",
    "                                  x_train[k]).predict_proba(z_val_inter[k])\n",
    "        cv_er_inter.append(-np.sum(np.log(p_cv_inter))/len(p_cv_inter))\n",
    "\n",
    "        # Encoded categorical\n",
    "        p_cv_enc = lg_inter.fit(z_train_enc[k],\n",
    "                                x_train[k]).predict_proba(z_val_enc[k])\n",
    "        cv_er_enc.append(-np.sum(np.log(p_cv_enc))/len(p_cv_enc))\n",
    "\n",
    "    cv_er_lg = np.mean(cv_er_lg)\n",
    "    cv_er_lasso = np.mean(cv_er_lasso)\n",
    "    cv_er_inter = np.mean(cv_er_inter)\n",
    "    cv_er_enc = np.mean(cv_er_enc)\n",
    "\n",
    "    # Point\n",
    "    cv_er_tree = []\n",
    "    cv_er_boost = []\n",
    "    for k in range(k_):\n",
    "        # Tree\n",
    "        cm_tree_cv =\\\n",
    "            confusion_matrix(x_val[k],\n",
    "                             tree_clf.fit(z_train[k],\n",
    "                                          x_train[k]).predict(z_val[k]))\n",
    "        er_tree_cv = (cm_tree_cv[0, 1]/np.sum(x_val[k] == 0) +\n",
    "                      cm_tree_cv[1, 0]/np.sum(x_val[k] == 1))  # error\n",
    "        cv_er_tree.append(er_tree_cv)\n",
    "\n",
    "        # Gradient boosting\n",
    "        cm_boost_cv =\\\n",
    "            confusion_matrix(x_val[k],\n",
    "                             boost_clf.fit(z_train[k],\n",
    "                                           x_train[k]).predict(z_val[k]))\n",
    "        er_boost_cv = (cm_boost_cv[0, 1]/np.sum(x_val[k] == 0) +\n",
    "                       cm_boost_cv[1, 0]/np.sum(x_val[k] == 1))  # error\n",
    "        cv_er_boost.append(er_boost_cv)\n",
    "\n",
    "    cv_er_tree = np.mean(cv_er_tree)\n",
    "    cv_er_boost = np.mean(cv_er_boost)\n",
    "\n",
    "    print('Logistic CV error: %1.3f' % cv_er_lg)\n",
    "    print('Logistic with interactions CV error: %1.3f' % cv_er_inter)\n",
    "    print('Logistic with interactions and categorical CV error: %1.3f' %\n",
    "          cv_er_enc)\n",
    "    print('Logistic with lasso CV error: %1.3f' % cv_er_lasso)\n",
    "    print('CART classifier CV error: %1.3f' % cv_er_tree)\n",
    "    print('CART classifier with gradient boosting CV error: %1.3f' %\n",
    "          cv_er_boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('arpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax11 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax12 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax13 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax11)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_lg, tpr_lg, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_lg)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_lg)\n",
    "plt.title('Logistic regression (test set)')\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax12)\n",
    "plt.hist(s_0_lg, 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_lg, 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax13)\n",
    "cax_1 = plt.bar([0, 1], [cm_lg[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_lg[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "add_logo(fig1, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Logistic regression with interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax31 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax32 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax33 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax31)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_inter, tpr_inter, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_inter)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_inter)\n",
    "plt.title('Logistic regression with interactions deg. = %1i (test set)'\n",
    "          % pol_degree)\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax32)\n",
    "plt.hist(s_0_inter, 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_inter, 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax33)\n",
    "cax_1 = plt.bar([0, 1], [cm_inter[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_inter[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "add_logo(fig2, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Logistic regression with interactions and encoded categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plt.figure()\n",
    "ax21 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax22 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax23 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax21)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_enc, tpr_enc, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_enc)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_enc)\n",
    "plt.title('Logistic regression with interactions and categorical features')\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax22)\n",
    "plt.hist(s_0_enc, 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_enc, 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax23)\n",
    "cax_1 = plt.bar([0, 1], [cm_enc[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_enc[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "add_logo(fig3, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Logistic regression with lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plt.figure()\n",
    "ax21 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax22 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax23 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax21)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_lasso, tpr_lasso, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_lasso)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_lasso)\n",
    "plt.title('Logistic regression with Lasso param. = %1.2e (test set)' %\n",
    "          lambda_lasso)\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax22)\n",
    "plt.hist(s_0_lasso, 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_lasso, 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax23)\n",
    "cax_1 = plt.bar([0, 1], [cm_lasso[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_lasso[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "add_logo(fig4, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) CART classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plt.figure()\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax1)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_tree, tpr_tree, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_tree)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_tree)\n",
    "plt.title('CART classifier: max. depth of tree = %1i (test set)'\n",
    "          % max_depth_tree)\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax2)\n",
    "plt.hist(s_0_tree[~np.isinf(s_0_tree)], 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_tree[~np.isinf(s_1_tree)], 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax3)\n",
    "cax_1 = plt.bar([0, 1], [cm_tree[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_tree[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "add_logo(fig5, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6 = plt.figure()\n",
    "# Parameters\n",
    "n_classes = 2\n",
    "plot_colors = \"rb\"\n",
    "plot_step = 0.2\n",
    "\n",
    "k1 = -10\n",
    "k2 = -12\n",
    "\n",
    "z_k1_min = z_estimation[:, k1].min()\n",
    "z_k1_max = z_estimation[:, k1].max()\n",
    "z_k2_min = z_estimation[:, k2].min()\n",
    "z_k2_max = z_estimation[:, k2].max()\n",
    "zz_k1, zz_k2 = np.meshgrid(np.arange(z_k1_min, z_k1_max, plot_step),\n",
    "                           np.arange(z_k2_min, z_k2_max, plot_step))\n",
    "tree_clf_plot = tree.DecisionTreeClassifier(max_depth=max_depth_tree)\n",
    "p_plot = tree_clf_plot.fit(z_estimation[:, [k1, k2]],\n",
    "                           x_estimation).predict_proba(np.c_[zz_k1.ravel(),\n",
    "                                                       zz_k2.ravel()])[:, 1]\n",
    "p_plot = p_plot.reshape(zz_k1.shape)\n",
    "cs = plt.contourf(zz_k1, zz_k2, p_plot, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(x_estimation == i)\n",
    "    plt.scatter(z_estimation[idx, k1], z_estimation[idx, k2], c=color,\n",
    "                label=['0', '1'][i],\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.xlabel(list(df)[k1])\n",
    "plt.ylabel(list(df)[k2])\n",
    "plt.xlim([z_k1_min, z_k1_max])\n",
    "plt.ylim([z_k2_min, z_k2_max])\n",
    "plt.title('CART classifier decision regions')\n",
    "add_logo(fig6, alpha=0.8, location=3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7 = plt.figure()\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "\n",
    "# out of sample ROC curve\n",
    "plt.sca(ax1)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g')\n",
    "plt.plot(fpr_boost, tpr_boost, 'b')\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Random fit', 'Perfect fit', 'ROC curve'])\n",
    "plt.text(0.05, 0.8, 'AUC = %.2f' % auc_tree)\n",
    "plt.text(0.05, 0.85, 'Error = %.2f' % er_tree)\n",
    "plt.title('CART classifier with gradient boosting (test set)')\n",
    "\n",
    "# Scores\n",
    "plt.sca(ax2)\n",
    "plt.hist(s_0_boost, 80, density=True, alpha=0.7, color='r')\n",
    "plt.hist(s_1_boost, 80, density=True, alpha=0.7, color='b')\n",
    "plt.legend(['S | 0', 'S | 1'])\n",
    "plt.title('Scores distribution')\n",
    "\n",
    "# Confusion matrix\n",
    "plt.sca(ax3)\n",
    "cax_1 = plt.bar([0, 1], [cm_boost[0, 1]/np.sum(x_test == 0),\n",
    "                         cm_boost[1, 0]/np.sum(x_test == 1)])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks([0, 1], ('$fpr$', '$fnr$'))\n",
    "plt.title('Confusion matrix')\n",
    "\n",
    "add_logo(fig7, location=1, size_frac_x=1/8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8 = plt.figure()\n",
    "# Parameters\n",
    "n_classes = 2\n",
    "plot_colors = \"rb\"\n",
    "plot_step = 0.2\n",
    "\n",
    "k1 = -10\n",
    "k2 = -12\n",
    "\n",
    "z_k1_min = z_estimation[:, k1].min()\n",
    "z_k1_max = z_estimation[:, k1].max()\n",
    "z_k2_min = z_estimation[:, k2].min()\n",
    "z_k2_max = z_estimation[:, k2].max()\n",
    "zz_k1, zz_k2 = np.meshgrid(np.arange(z_k1_min, z_k1_max, plot_step),\n",
    "                           np.arange(z_k2_min, z_k2_max, plot_step))\n",
    "boost_clf_plot = GradientBoostingClassifier()\n",
    "p_plot = boost_clf_plot.fit(z_estimation[:, [k1, k2]],\n",
    "                            x_estimation).predict_proba(np.c_[zz_k1.ravel(),\n",
    "                                                        zz_k2.ravel()])[:, 1]\n",
    "p_plot = p_plot.reshape(zz_k1.shape)\n",
    "cs = plt.contourf(zz_k1, zz_k2, p_plot, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "for i, color in zip(range(n_classes), plot_colors):\n",
    "    idx = np.where(x_estimation == i)\n",
    "    plt.scatter(z_estimation[idx, k1], z_estimation[idx, k2], c=color,\n",
    "                label=['0', '1'][i],\n",
    "                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.xlabel(list(df)[k1])\n",
    "plt.ylabel(list(df)[k2])\n",
    "plt.xlim([z_k1_min, z_k1_max])\n",
    "plt.ylim([z_k2_min, z_k2_max])\n",
    "plt.title('CART classifier with gradient boosting decision regions')\n",
    "add_logo(fig8, alpha=0.8, location=3)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
