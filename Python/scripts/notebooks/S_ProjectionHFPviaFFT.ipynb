{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_ProjectionHFPviaFFT [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=S_ProjectionHFPviaFFT&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=ExerHFPProj)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "sys.path.append(path.abspath('../../functions-legacy'))\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange, reshape, ones, cumsum, diff, abs, log, exp, sqrt, array, r_\n",
    "from numpy import sum as npsum, min as npmin, max as npmax\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, plot, legend, ylabel, \\\n",
    "    xlabel, title\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from CONFIG import GLOBAL_DB, TEMPORARY_DB\n",
    "from ARPM_utils import struct_to_dict, datenum, save_plot\n",
    "from FPmeancov import FPmeancov\n",
    "from intersect_matlab import intersect\n",
    "from EffectiveScenarios import EffectiveScenarios\n",
    "from ConditionalFP import ConditionalFP\n",
    "from ProjDFFT import ProjDFFT\n",
    "from SampleScenProbDistribution import SampleScenProbDistribution\n",
    "\n",
    "# parameters\n",
    "j_ = 10 ** 4  # Number of scenarios\n",
    "deltat = 20  # investment horizon expressed in days\n",
    "tau_HL = 1260  # Half life probability expressed in days\n",
    "alpha = 0.35  # probability range\n",
    "k_ = 2 ** 12  # coarseness level for projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_OptionStrategy'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_OptionStrategy'), squeeze_me=True)\n",
    "\n",
    "OptionStrategy = struct_to_dict(db['OptionStrategy'])\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_VIX'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_VIX'), squeeze_me=True)\n",
    "\n",
    "VIX = struct_to_dict(db['VIX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the realized time series of daily P&L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invariants (daily P&L)\n",
    "pi = OptionStrategy.cumPL  # cumulative P&L\n",
    "epsi = diff(pi)  # daily P&L\n",
    "dates_x = array([datenum(i) for i in OptionStrategy.Dates])\n",
    "dates_x = dates_x[1:]\n",
    "\n",
    "# conditioning variable (VIX)\n",
    "z = VIX.value\n",
    "dates_z = VIX.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersect the time series of daily P&L and VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates, i_epsi, i_z = intersect(dates_x, dates_z)\n",
    "\n",
    "pi = pi[i_epsi + 1]\n",
    "epsi = epsi[i_epsi]\n",
    "z = z[i_z]\n",
    "t_ = len(epsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Minimum Relative Entropy Pooling conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior\n",
    "lam = log(2) / tau_HL  # half life 5y\n",
    "prior = exp(-lam*abs(arange(t_, 1 + -1, -1))).reshape(1,-1)\n",
    "prior = prior / npsum(prior)\n",
    "\n",
    "# conditioner\n",
    "VIX = namedtuple('VIX', 'Series TargetValue Leeway')\n",
    "VIX.Series = z.reshape(1,-1)\n",
    "VIX.TargetValue = np.atleast_2d(z[-1])\n",
    "VIX.Leeway = alpha\n",
    "\n",
    "# flexible probabilities conditioned via EP\n",
    "p = ConditionalFP(VIX, prior)\n",
    "p[p == 0] = 10 ** -20  # avoid log[0-1] in ens computation\n",
    "p = p /npsum(p)\n",
    "\n",
    "# effective number of scenarios\n",
    "typ = namedtuple('type','Entropy')\n",
    "typ.Entropy = 'Exp'\n",
    "ens = EffectiveScenarios(p, typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the HFP-estimators of location and dispersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma2 = FPmeancov(epsi.reshape(1,-1), p)\n",
    "sigma = sqrt(sigma2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project the HFP-pdf to the horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mu_hat, sigma2_hat] = FPmeancov(epsi.reshape(1,-1), p)\n",
    "sigma_hat = sqrt(sigma2_hat)\n",
    "epsi_hat = (epsi - mu_hat) / sigma_hat\n",
    "\n",
    "xi, f_hat_deltat,*_ = ProjDFFT(epsi_hat, p, deltat, k_)\n",
    "epsi_deltat = mu_hat*deltat + sigma_hat*xi\n",
    "f_deltat = f_hat_deltat / sigma_hat\n",
    "f_deltat = np.real(f_deltat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a normal approximation of the projected HFP-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_deltat = norm.pdf(epsi_deltat, mu*deltat, sigma*sqrt(deltat))\n",
    "\n",
    "# center around last realization of pi\n",
    "epsi_deltat = epsi_deltat + pi[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) compute low-order central moments at the horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute one-step FP-moments\n",
    "mu_1 = mu.squeeze()  # expected value\n",
    "sigma_1 = sigma.squeeze()  # standard deviation\n",
    "varsigma_1 = p@(epsi.reshape(-1,1) - mu_1) ** 3 / (sigma_1 ** 3)  # skewness\n",
    "kappa_1 = p@(epsi.reshape(-1,1) - mu_1) ** 4 / (sigma ** 4) - 3  # excess kurtosis\n",
    "\n",
    "# project FP-moments to horizon tau\n",
    "mu_tau = pi[-1] + mu_1*deltat  # projected (shifted) expexted value\n",
    "sigma_tau = sigma_1*sqrt(deltat)  # projected standard deviation\n",
    "varsigma_tau = varsigma_1 / sqrt(deltat)  # projected skewness\n",
    "kappa_tau = kappa_1 / deltat  # projected excess kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate scenarios of projected path risk drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenarios for the invariants via historical bootstrapping\n",
    "epsi_hor = SampleScenProbDistribution(epsi.reshape(1,-1), p, j_*deltat)\n",
    "epsi_hor = reshape(epsi_hor, (j_, deltat),'F')\n",
    "\n",
    "# Feed the simulated scenarios in the recursive incremental-step routine((random walk assumption))\n",
    "pi_deltat = pi[-1] + cumsum(epsi_hor, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ = 2  # number of plotted observation before projecting time\n",
    "\n",
    "# axes settings\n",
    "m = min([npmin(pi[::-1]), pi[-1]-4*sigma_tau])\n",
    "M = max([npmax(pi[::-1]), mu_tau + 4.5*sigma_tau])\n",
    "t = arange(-s_,deltat+1)\n",
    "max_scale = deltat / 4\n",
    "\n",
    "# preliminary computations\n",
    "tau_red = arange(0,deltat+0.1,0.1)\n",
    "mu_red = pi[-1] + mu_1*tau_red\n",
    "sigma_red = sigma_1*sqrt(tau_red)\n",
    "redline1 = mu_red + 2*sigma_red\n",
    "redline2 = mu_red - 2*sigma_red\n",
    "\n",
    "from matplotlib.pyplot import xticks\n",
    "\n",
    "f = figure()\n",
    "# color settings\n",
    "lgrey = [0.8, 0.8, 0.8]  # light grey\n",
    "dgrey = [0.2, 0.2, 0.2]  # dark grey\n",
    "lblue = [0.27, 0.4, 0.9]  # light blue\n",
    "plt.axis([t[0], t[-1] + max_scale, m, M])\n",
    "xlabel('time (days)')\n",
    "ylabel('Risk driver')\n",
    "plt.xticks(arange(-2,21))\n",
    "plt.grid(False)\n",
    "title('Historical process with Flexible Probabilities')\n",
    "# standard deviation lines\n",
    "p_red_1 = plot(tau_red, redline1, color='r', lw=2)  # red bars (+2 std dev)\n",
    "p_red_2 = plot(tau_red, redline2, color='r', lw=2)  # red bars (-2std dev)\n",
    "p_mu = plot([0, deltat], [pi[-1], mu_tau], color='g', lw = 2)  # expectation\n",
    "# histogram pdf plot\n",
    "for k in range(f_deltat.shape[1]):\n",
    "    plot([deltat, deltat + f_deltat[0,k]], [epsi_deltat[0,k], epsi_deltat[0,k]], color=lgrey, lw=2)\n",
    "f_border = plot(deltat + f_deltat.T, epsi_deltat.T, color=dgrey, lw=1)\n",
    "# normal approximation plot\n",
    "phi_border = plot(deltat + phi_deltat.T, epsi_deltat.T, color=lblue, lw=1)\n",
    "# plot of last s_ observations\n",
    "for k in range(s_):\n",
    "    plot([t[k], t[k+1]], [pi[-s_+k-1], pi[-s_+k]], color=lgrey, lw=2)\n",
    "    plot(t[k], pi[-s_+k-1], color='b',linestyle='none', marker='.',markersize=15)\n",
    "plot(t[s_], pi[-1], color='b',linestyle='none', marker = '.',markersize=15)\n",
    "# paths\n",
    "plot(arange(deltat+1), r_['-1',pi[-1]*ones((20, 1)), pi_deltat[:20,:]].T, color= lgrey, lw=1,zorder=0)\n",
    "# leg\n",
    "legend(handles=[f_border[0], phi_border[0], p_mu[0], p_red_1[0]],\n",
    "       labels=['horizon pdf','normal approximation','expectation',' + / - 2 st.deviation']);\n",
    "# save_plot(ax=plt.gca(), extension='png', scriptname=os.path.basename('.')[:-3], count=plt.get_fignums()[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
