{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_ProjectionOptionHFP [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=S_ProjectionOptionHFP&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=eb-proj-hist-dist-fpnew)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "sys.path.append(path.abspath('../../functions-legacy'))\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange, reshape, zeros, where, cumsum, diff, abs, round, mean, log, exp, sqrt, tile, r_, atleast_2d, \\\n",
    "    newaxis, array\n",
    "from numpy import sum as npsum, max as npmax\n",
    "\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure, bar, xlim, ylim, scatter, ylabel, \\\n",
    "    xlabel, title, xticks, yticks\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from CONFIG import GLOBAL_DB, TEMPORARY_DB\n",
    "from ARPM_utils import save_plot, struct_to_dict\n",
    "from FPmeancov import FPmeancov\n",
    "from intersect_matlab import intersect\n",
    "from PlotTwoDimEllipsoid import PlotTwoDimEllipsoid\n",
    "from HistogramFP import HistogramFP\n",
    "from RollPrices2YieldToMat import RollPrices2YieldToMat\n",
    "from EffectiveScenarios import EffectiveScenarios\n",
    "from ConditionalFP import ConditionalFP\n",
    "from Delta2MoneynessImplVol import Delta2MoneynessImplVol\n",
    "from ColorCodedFP import ColorCodedFP\n",
    "from HFPquantile import HFPquantile\n",
    "from InverseCallTransformation import InverseCallTransformation\n",
    "\n",
    "# parameters\n",
    "tau = 6  # projection horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload databases db_ImpliedVol_SPX, db_SwapCurve and db_VIX, and where the common daily observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_ImpliedVol_SPX'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_ImpliedVol_SPX'), squeeze_me=True)  # underlying values and implied volatility surface for S&P 500\n",
    "\n",
    "db_ImpliedVol_SPX = struct_to_dict(db['db_ImpliedVol_SPX'], False)\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_SwapCurve'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_SwapCurve'),\n",
    "                 squeeze_me=True)  # rolling values used to computed the short rate\n",
    "\n",
    "DF_Rolling = struct_to_dict(db['DF_Rolling'], False)\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_VIX'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_VIX'), squeeze_me=True)  # Vix index values\n",
    "\n",
    "VIX = struct_to_dict(db['VIX'],False)\n",
    "\n",
    "# where the common observations between db_ImpliedVol_SPX (thus obtaining a\n",
    "# reduced db_ImpliedVol_SPX database) and DF_Rolling (thus obtaining a reduced DF_Rolling database)\n",
    "[_, i_impvol, i_rates] = intersect(db_ImpliedVol_SPX['Dates'], DF_Rolling['Dates'])\n",
    "db_ImpliedVol_SPX['Dates'] = db_ImpliedVol_SPX['Dates'][i_impvol]\n",
    "db_ImpliedVol_SPX['Underlying'] = db_ImpliedVol_SPX['Underlying'][i_impvol]\n",
    "db_ImpliedVol_SPX['Sigma'] = db_ImpliedVol_SPX['Sigma'][:,:, i_impvol]\n",
    "DF_Rolling['Dates'] = DF_Rolling['Dates'][i_rates]\n",
    "DF_Rolling['Prices'] = DF_Rolling['Prices'][:, i_rates]\n",
    "\n",
    "# where the common observations between the reduced db_ImpliedVol_SPX database\n",
    "# (thus obtaining a new reduced db_ImpliedVol_SPX database) and db_VIX (thus obtaining a reduced db_VIX database)\n",
    "[dates, i_impvol, i_vix] = intersect(db_ImpliedVol_SPX['Dates'], VIX['Date'])\n",
    "VIX['Date'] = VIX['Date'][i_vix]\n",
    "VIX['value'] = VIX['value'][i_vix]\n",
    "db_ImpliedVol_SPX['Dates'] = db_ImpliedVol_SPX['Dates'][i_impvol]\n",
    "db_ImpliedVol_SPX['Underlying'] = db_ImpliedVol_SPX['Underlying'][i_impvol]\n",
    "db_ImpliedVol_SPX['Sigma'] = db_ImpliedVol_SPX['Sigma'][:,:, i_impvol]\n",
    "\n",
    "# where the observations in the reduced DF_Rolling database which are common\n",
    "# to the new reduced db_ImpliedVol_SPX database and the reduced db_VIX database\n",
    "DF_Rolling['Dates'] = DF_Rolling['Dates'][i_impvol]\n",
    "DF_Rolling['Prices'] = DF_Rolling['Prices'][:, i_impvol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the risk drivers, i.e. the log value of the underlying, the short shadow rate and the log-implied volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk driver: the log-value of S&P 500\n",
    "underlying = db_ImpliedVol_SPX['Underlying']\n",
    "x_1 = log(underlying)\n",
    "\n",
    "# risk driver: the short shadow rate\n",
    "tau_shortrate = 0.3333  # time to maturity of the short rate (4 months)\n",
    "eta = 0.013  # inverse-call parameter\n",
    "index_shortrate = where(DF_Rolling['TimeToMat'] == tau_shortrate)\n",
    "shortrate,_ = RollPrices2YieldToMat(DF_Rolling['TimeToMat'][index_shortrate], DF_Rolling['Prices'][index_shortrate,:])\n",
    "x_2 = InverseCallTransformation(shortrate, {1:eta}).squeeze()\n",
    "y = mean(shortrate)\n",
    "\n",
    "# risk driver: the logarithm of the implied volatility\n",
    "maturity = db_ImpliedVol_SPX['TimeToMaturity']\n",
    "delta = db_ImpliedVol_SPX['Delta']  # delta-moneyness\n",
    "sigma_delta = db_ImpliedVol_SPX['Sigma']\n",
    "n_ = len(maturity)\n",
    "k_ = len(delta)\n",
    "t_x = sigma_delta.shape[2]  # number of risk drivers scenarios\n",
    "\n",
    "# construct the moneyness grid\n",
    "max_m = 0.3\n",
    "min_m = -0.3\n",
    "n_grid = 6\n",
    "m_grid = min_m + (max_m - min_m) * arange(n_grid + 1) / n_grid\n",
    "\n",
    "# m-parametrized log-implied volatility surface\n",
    "sigma_m = zeros((n_, n_grid + 1, t_x))\n",
    "for t in range(t_x):\n",
    "    for n in range(n_):\n",
    "        sigma_m[n,:,t],*_ = Delta2MoneynessImplVol(sigma_delta[n,:, t], delta, maturity[n], y, m_grid)\n",
    "\n",
    "x_3 = log(reshape(sigma_m, (n_*(n_grid + 1), t_x),'F'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the historical daily invariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_1 = diff(x_1)\n",
    "epsilon_2 = diff(x_2)\n",
    "epsilon_3 = diff(x_3, 1, 1)\n",
    "\n",
    "t_ = len(epsilon_1)  # number of daily invariants scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the scenarios for the paths of the overlapping invariants for tau=1,...,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage\n",
    "j_ = t_ - tau + 1  # number of overlapping invariants series\n",
    "epsilon_1overlap = zeros((j_, tau))\n",
    "epsilon_2overlap = zeros((j_, tau))\n",
    "epsilon_3overlap = zeros(((n_grid + 1)*n_, j_, tau))\n",
    "\n",
    "# overlapping series approach\n",
    "for j in range(j_):\n",
    "    # j-th path of the invariants\n",
    "    epsilon_1overlap[j,:] = cumsum(epsilon_1[j: j + tau])\n",
    "    epsilon_2overlap[j,:] = cumsum(epsilon_2[j: j + tau])\n",
    "    epsilon_3overlap[:, j,:] = cumsum(epsilon_3[:, j: j + tau], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Flexible Probabilities via smoothing and scoring on VIX log return\n",
    "## and compute the effective number of scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIX value\n",
    "v_VIX = VIX['value']\n",
    "# VIX compounded returns\n",
    "c = diff(log(v_VIX))\n",
    "# Compute the time series of the risk factor by applying sequentially smoothing and scoringfilters to the time series the VIX index compounded return\n",
    "# smoothing\n",
    "z = zeros(t_)\n",
    "times = range(t_)\n",
    "tauHL_smoo = 15  # fast half-life time\n",
    "for t in range(t_):\n",
    "    p_smoo_t = exp(-log(2) / tauHL_smoo*(tile(t+1, (1, t+1))-times[:t+1]))\n",
    "    gamma_t = npsum(p_smoo_t)\n",
    "    z[t] = npsum(p_smoo_t * c[:t+1]) / gamma_t\n",
    "\n",
    "# scoring\n",
    "mu_hat = zeros(t_)\n",
    "mu2_hat = zeros(t_)\n",
    "sd_hat = zeros(t_)\n",
    "tauHL_scor = 100  # slow half-life time\n",
    "for t in range(t_):\n",
    "    p_scor_t = exp(-log(2) / tauHL_scor*(tile(t+1, (1, t+1))-times[:t+1]))\n",
    "    gamma_scor_t = npsum(p_scor_t)\n",
    "    mu_hat[t] = npsum(p_scor_t * z[:t+1]) / gamma_scor_t\n",
    "    mu2_hat[t] = npsum(p_scor_t * z[:t+1]** 2) / gamma_scor_t\n",
    "    sd_hat[t] = sqrt(mu2_hat[t]-(mu_hat[t]) ** 2)\n",
    "\n",
    "z = (z - mu_hat) / sd_hat\n",
    "z[0] = mu_hat[0]\n",
    "\n",
    "# conditioner\n",
    "VIX = namedtuple('VIX', 'Series TargetValue Leeway')\n",
    "VIX.Series = z.reshape(1,-1)  # time series of the conditioning variable (log return of VIX quotations)\n",
    "VIX.TargetValue = atleast_2d(z[-1])  # target value for the conditioner\n",
    "VIX.Leeway = 0.3  # (alpha) probability contained in the range\n",
    "\n",
    "# prior set of probabilities\n",
    "tau_HL = 252*4  # (half life) 4 years\n",
    "prior = exp(-log(2) / tau_HL*abs(arange(VIX.Series.shape[1],0,-1))).reshape(1,-1)\n",
    "prior = prior / npsum(prior)\n",
    "\n",
    "# Flexible Probabilities conditioned via entropy pooling\n",
    "p_all = ConditionalFP(VIX,prior)  # Flexible Probabilities conditioned on the VIX log return, for each day corresponding to the invariants'ime series\n",
    "\n",
    "p = zeros((1,j_))\n",
    "\n",
    "for j in range(j_):\n",
    "    # The flexible probability of the j_th scenario is (proportional to) the average of the probabilities of the tau invariants in the corresponding overlapping series\n",
    "    p[0,j]=npsum(p_all[0,j:j + tau]) / tau\n",
    "\n",
    "p = p /npsum(p)\n",
    "\n",
    "# effective number of scenarios\n",
    "typ = namedtuple('type','Entropy')\n",
    "typ.Entropy = 'Exp'\n",
    "ens = EffectiveScenarios(p, typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the scenarios for the paths of the risk drivers by applying the projection formula for tau=1,...,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1hor = x_1[-1] + epsilon_1overlap\n",
    "x_2hor = x_2[-1] + epsilon_2overlap\n",
    "x_3hor = tile(x_3[:,[-1],newaxis], [1, j_, tau]) + epsilon_3overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data in db_ProjOptionsHFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varnames_to_save = [x_1,j_,x_1hor,x_2,x_2hor,x_3,x_3hor,n_,n_grid,tau,eta,sigma_m ,maturity,m_grid,p,ens,sigma_m,dates]\n",
    "# vars_to_save = {varname: var for varname, var in locals().items() if isinstance(var,(np.ndarray,np.float,np.int)) and varname in varnames_to_save}\n",
    "# savemat(os.path.join(TEMPORARY_DB, 'db_ProjOptionsHFP'),vars_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the horizon for the plot select the log-underlying and the log- ATM 1yr impl vol compute the HFP mean and covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1fixhor = x_1hor[:,[-1]]\n",
    "mateq1 = where(maturity==1)[0]+1\n",
    "mgrideq0 = where(m_grid==0)[0]+1\n",
    "x_3fixhor = x_3hor[mateq1*mgrideq0-1,:, [-1]].T\n",
    "\n",
    "[mu_HFP, sigma2_HFP] = FPmeancov(r_['-1',x_1fixhor, x_3fixhor].T, p)\n",
    "\n",
    "col = [0.94, 0.3, 0]\n",
    "colhist = [.9, .9, .9]\n",
    "# axis settings\n",
    "x1_l = HFPquantile(x_1fixhor.T, array([[10 ** -6]]), p).squeeze()\n",
    "x1_u = HFPquantile(x_1fixhor.T, array([[1 - 10 ** -6]]), p).squeeze()\n",
    "x2_l = HFPquantile(x_3fixhor.T, array([[10 ** -6]]), p).squeeze()\n",
    "x2_u = HFPquantile(x_3fixhor.T, array([[1 - 10 ** -6]]), p).squeeze()\n",
    "\n",
    "f = figure()\n",
    "grey_range = arange(0,0.81,0.01)\n",
    "CM, C = ColorCodedFP(p, None, None, grey_range, 0, 1, [0.75, 0.25])\n",
    "# colormap(CM)\n",
    "option = namedtuple('option', 'n_bins')\n",
    "option.n_bins = round(6*log(ens))\n",
    "n1, c1 = HistogramFP(x_1fixhor.T, p, option)\n",
    "option = namedtuple('option', 'n_bins')\n",
    "option.n_bins = round(7*log(ens))\n",
    "n2, c2 = HistogramFP(x_3fixhor.T, p, option)\n",
    "coeff = 1\n",
    "plt.subplot2grid((4,4),(1,3),rowspan=3)\n",
    "plt.barh(c2[:-1], n2[0], height=c2[1]-c2[0], facecolor= colhist, edgecolor= 'k')\n",
    "plt.axis([0, npmax(n2) + npmax(n2) / 20,x2_l, x2_u])\n",
    "xticks([])\n",
    "yticks([])\n",
    "plt.subplot2grid((4,4),(0,0),colspan=3)\n",
    "bar(c1[:-1], n1[0], width=c1[1]-c1[0], facecolor= colhist, edgecolor= 'k')\n",
    "plt.axis([x1_l, x1_u, 0, npmax(n1) + npmax(n1) / 20])\n",
    "xticks([])\n",
    "yticks([])\n",
    "plt.title('Historical Distribution with Flexible Probabilities horizon= {horizon} days'.format(horizon=tau))\n",
    "plt.subplot2grid((4,4),(1,0),colspan=3, rowspan=3)\n",
    "X = x_1fixhor\n",
    "Y = x_3fixhor\n",
    "scatter(X, Y, 30, c=C, marker='.',cmap=CM)\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().xaxis.set_label_position(\"top\")\n",
    "xlim([x1_l, x1_u])\n",
    "ylim([x2_l, x2_u])\n",
    "xlabel('$X_1$')\n",
    "ylabel('$x_3$')\n",
    "plt.gca().yaxis.tick_right()\n",
    "plt.gca().yaxis.set_label_position(\"right\")\n",
    "PlotTwoDimEllipsoid(mu_HFP, sigma2_HFP, 1, 0, 0, col, 2);\n",
    "plt.tight_layout()\n",
    "# save_plot(ax=plt.gca(), extension='png', scriptname=os.path.basename('.')[:-3], count=plt.get_fignums()[-1])\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
