{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s_estimation_copmarg_calloption [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=s_estimation_copmarg_calloption&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=ExerCopulaEstim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t as tstu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from arpym.statistics.cop_marg_sep import cop_marg_sep\n",
    "from arpym.statistics.mvt_pdf import mvt_pdf\n",
    "from arpym.statistics.scoring import scoring\n",
    "from arpym.statistics.smoothing import smoothing\n",
    "from arpym.estimation.conditional_fp import conditional_fp\n",
    "from arpym.estimation.cov_2_corr import cov_2_corr\n",
    "from arpym.estimation.effective_num_scenarios import effective_num_scenarios\n",
    "from arpym.estimation.exp_decay_fp import exp_decay_fp\n",
    "from arpym.estimation.factor_analysis_paf import factor_analysis_paf\n",
    "from arpym.estimation.fit_garch_fp import fit_garch_fp\n",
    "from arpym.estimation.fit_locdisp_mlfp import fit_locdisp_mlfp\n",
    "from arpym.tools.logo import add_logo\n",
    "from arpym.tools.histogram_sp import histogram_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Input parameters](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_hl_prior = 4*252  # half-life parameter for time conditioning\n",
    "tau_hl_smooth = 21  # half-life parameter for VIX smoothing\n",
    "tau_hl_score = 5*21  # half-life parameter for VIX scoring\n",
    "alpha = 0.5 # proportion of obs. included in range for state conditioning\n",
    "nu_min = 2  # lower bound for the degrees of freedom for t copula\n",
    "nu_max = 20  # upper bound for the degrees of freedom for t copula\n",
    "i_plot = 1 # invariant chosed for the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 0](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-implementation-step00): Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIX (used for time-state conditioning)\n",
    "vix_path = '~/databases/global-databases/derivatives/db_vix/data.csv'\n",
    "db_vix = pd.read_csv(vix_path, usecols=['date', 'VIX_close'],\n",
    "                     index_col=0)\n",
    "db_vix.index = pd.to_datetime(db_vix.index)\n",
    "\n",
    "# S&P 500 index\n",
    "path = '~/databases/global-databases/equities/db_stocks_SP500/'\n",
    "db_sp500 = pd.read_csv(path+'SPX.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "path = '~/databases/temporary-databases/'\n",
    "\n",
    "# implied volatility (used for dates)\n",
    "db_calloption_rd = pd.read_csv(path+'db_calloption_rd.csv', index_col=0,\n",
    "                             parse_dates=True)\n",
    "dates = pd.to_datetime(np.array(db_calloption_rd.index))\n",
    "\n",
    "# invariants extracted from the log-implied volatility\n",
    "db_calloption_epsi_var1 = pd.read_csv(path+'db_calloption_epsi_var1.csv',\n",
    "                                 index_col=0, parse_dates=True)\n",
    "epsi_var1 = db_calloption_epsi_var1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 1](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-implementation-step01): Extract invariants for the S&P 500 index and create the realized information panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute risk driver for the S&P 500 index as the log-value\n",
    "log_underlying = \\\n",
    "    np.log(np.array(db_sp500.loc[(db_sp500.index.isin(dates)), 'SPX_close']))\n",
    "\n",
    "# model log_underlying as GARCH(1,1)\n",
    "par, sig2, epsi_garch = fit_garch_fp(np.diff(log_underlying))\n",
    "\n",
    "# store all the invariants in the realized information panel\n",
    "epsi = np.c_[epsi_garch, epsi_var1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 2](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-implementation-step02): Set the flexible probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_, i_ = epsi.shape\n",
    "# state indicator: VIX compounded return realizations\n",
    "c_vix = np.diff(np.log(np.array(db_vix.loc[dates].VIX_close)))\n",
    "# smoothing\n",
    "z_smooth = smoothing(c_vix, tau_hl_smooth)\n",
    "# scoring\n",
    "z = scoring(z_smooth, tau_hl_score)\n",
    "# target value\n",
    "z_star = z[-1]\n",
    "# prior probabilities\n",
    "p_prior = exp_decay_fp(t_, tau_hl_prior)\n",
    "# posterior probabilities\n",
    "p = conditional_fp(z, z_star, alpha, p_prior)\n",
    "# effective number of scenarios\n",
    "ens = effective_num_scenarios(p)\n",
    "\n",
    "print(\"Effective number of scenarios is \", int(round(ens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 3](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-implementation-step03): Static elliptical copula estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the invariants grades\n",
    "u, _, _ = cop_marg_sep(epsi, p)\n",
    "\n",
    "# grid for the degrees of freedom parameter\n",
    "nu_copula = np.arange(nu_min, nu_max+1)\n",
    "l_ = len(nu_copula)\n",
    "\n",
    "# initialize variables\n",
    "rho2_copula_vec = np.zeros((i_, i_, l_))\n",
    "llike_nu = np.zeros(l_)\n",
    "epsi_tilde = np.zeros((t_, i_, l_))\n",
    "\n",
    "for l in range(l_):\n",
    "    # t-distributed invariants\n",
    "    epsi_tilde[:, :, l] = tstu.ppf(u, nu_copula[l])\n",
    "\n",
    "    # maximum likelihood\n",
    "    _, sig2_copula =\\\n",
    "        fit_locdisp_mlfp(epsi_tilde[:, :, l], p=p, nu=nu_copula[l],\n",
    "                         threshold=10**-3, maxiter=1000)\n",
    "    \n",
    "    # compute correlation matrix\n",
    "    rho2_copula_vec[:, :, l], _ = cov_2_corr(sig2_copula)\n",
    "\n",
    "    # compute log-likelihood at times with no missing values\n",
    "    llike_nu[l] = np.sum(p * np.log(mvt_pdf(epsi, np.zeros(i_),\n",
    "                                            rho2_copula_vec[:, :, l],\n",
    "                                            nu_copula[l])))\n",
    "# choose nu that gives the highest log-likelihood\n",
    "l_max = np.argmax(llike_nu)\n",
    "nu = nu_copula[l_max]\n",
    "rho2 = rho2_copula_vec[:, :, l_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 4](https://www.arpm.co/lab/redirect.php?permalink=s_estimation_copmarg_calloption-implementation-step04): Save databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARCH(1,1) realized invariants\n",
    "out = pd.DataFrame({'epsi_log_underlying': epsi_garch}, index=dates[1:])\n",
    "\n",
    "out.index.name = 'dates'\n",
    "out.to_csv('~/databases/temporary-databases/db_calloption_epsi_garch.csv')\n",
    "del out\n",
    "\n",
    "# GARCH(1,1) model parameters\n",
    "out = pd.DataFrame({'a': pd.Series(par[0]),\n",
    "                    'b': pd.Series(par[1]),\n",
    "                    'c': pd.Series(par[2]),\n",
    "                    'mu': pd.Series(par[3]),\n",
    "                    'sig2prev': pd.Series(sig2[-1]),\n",
    "                    'x_tnow': pd.Series(log_underlying[-1]),\n",
    "                    'x_tnow-1': pd.Series(log_underlying[-2])})\n",
    "out.to_csv('~/databases/temporary-databases/db_calloption_garch.csv')\n",
    "del out\n",
    "\n",
    "\n",
    "# flexible probabilities, copula degrees of freedom and correlation matrix\n",
    "out = pd.DataFrame({'p':pd.Series(p),\n",
    "                    'rho2_'+str(0): pd.Series(rho2[0, :])})\n",
    "for i in range(1, i_):\n",
    "    out = out.join(pd.DataFrame({'rho2_'+str(i): pd.Series(rho2[:, i])}))\n",
    "out = out.join(pd.DataFrame({'nu': pd.Series(nu)}))\n",
    "out.to_csv('~/databases/temporary-databases/db_calloption_estimation.csv',\n",
    "          index=None)\n",
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('arpm')\n",
    "\n",
    "# marginal distribution\n",
    "fig = plt.figure(figsize=(1280/72, 720/72), dpi=72)\n",
    "\n",
    "f_eps, x_eps = histogram_sp(epsi[:, i_plot-1], p=p, k_=10 * np.log(t_))\n",
    "bar_width = x_eps[1] - x_eps[0]\n",
    "plt.bar(x_eps, f_eps.flatten(), width=bar_width, fc=[0.7, 0.7, 0.7],\n",
    "        edgecolor=[0.5, 0.5, 0.5])\n",
    "\n",
    "plt.title('Distribution of the selected invariant',\n",
    "          fontweight='bold', fontsize=20)\n",
    "plt.xlabel('Invariant', fontsize=17)\n",
    "add_logo(fig, location=1, set_fig_size=False)\n",
    "fig.tight_layout()\n",
    "\n",
    "# copula correlation matrix\n",
    "fig2 = plt.figure(figsize=(1280.0/72.0, 720.0/72.0), dpi=72.0)\n",
    "plt.imshow(rho2_copula_vec[:, :, l_max])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.title('Estimated correlation matrix', fontweight='bold', fontsize=20)\n",
    "add_logo(fig2, size_frac_x=0.8, location=9, alpha=0.8, set_fig_size=False)\n",
    "fig2.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
