{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_AggregatesEstimation [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=S_AggregatesEstimation&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=eb-aggr-cond-fac-est-vue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "sys.path.append(path.abspath('../../functions-legacy'))\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange, zeros, where, argsort, diag, eye, abs, log, exp, sqrt, tile, r_, maximum, array, diagflat, \\\n",
    "    diff\n",
    "from numpy import sum as npsum\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "np.seterr(all=\"ignore\")\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from ARPM_utils import struct_to_dict\n",
    "from CONFIG import GLOBAL_DB, TEMPORARY_DB\n",
    "from MaxLikelihoodFPLocDispT import MaxLikelihoodFPLocDispT\n",
    "from ConditionalFP import ConditionalFP\n",
    "from DiffLengthMLFP import DiffLengthMLFP\n",
    "from FactorAnalysis import FactorAnalysis\n",
    "from pcacov import pcacov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Input parameters](https://www.arpm.co/lab/redirect.php?permalink=S_AggregatesEstimation-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauHL_smoo = 30  # half-life time for smoothing\n",
    "tauHL_scor = 100  # half-life time for scoring\n",
    "\n",
    "alpha = 0.25\n",
    "tauHL_prior = 21 * 4  # parameters for Flexible Probabilities conditioned on VIX\n",
    "\n",
    "nu_vec = range(2, 31)\n",
    "nu_ = len(nu_vec)\n",
    "\n",
    "nu_c1 = 12\n",
    "nu_c3 = 20\n",
    "nu_aggr = 5\n",
    "\n",
    "k_c1 = 4\n",
    "k_c3 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_Aggregates'))\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_Aggregates'))\n",
    "try:\n",
    "    dbvix = loadmat(os.path.join(GLOBAL_DB, 'db_VIX'))\n",
    "except FileNotFoundError:\n",
    "    dbvix = loadmat(os.path.join(TEMPORARY_DB, 'db_VIX'))\n",
    "\n",
    "epsi_c1 = db['epsi_c1']\n",
    "epsi_c3 = db['epsi_c3']\n",
    "dates = db['dates']\n",
    "\n",
    "VIX = struct_to_dict(dbvix['VIX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time series of the conditioning variable by applying sequentially smoothing and scoring filters to the time series of VIX's compounded returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_VIX = diff(log(VIX.value)).reshape(1,-1)\n",
    "t_vix = c_VIX.shape[1]\n",
    "times = range(t_vix)\n",
    "\n",
    "# smoothing\n",
    "z_vix = zeros((1, t_vix))\n",
    "for it in range(t_vix):\n",
    "    p_smoo_t = exp(-log(2) / tauHL_smoo * (tile(it + 1, (1, it + 1)) - times[:it + 1]))\n",
    "    gamma_t = npsum(p_smoo_t)\n",
    "    z_vix[0, it] = npsum(p_smoo_t * c_VIX[0, :it + 1]) / gamma_t\n",
    "\n",
    "# scoring\n",
    "mu_hat = zeros((1, t_vix))\n",
    "mu2_hat = zeros((1, t_vix))\n",
    "sd_hat = zeros((1, t_vix))\n",
    "for it in range(t_vix):\n",
    "    p_scor_t = exp(-log(2)/ tauHL_scor*(tile(it+1, (1, it+1)) - times[:it+1]))\n",
    "    gamma_scor_t = npsum(p_scor_t)\n",
    "    mu_hat[0, it] = npsum(p_scor_t * z_vix[0, :it+1]) / gamma_scor_t\n",
    "    mu2_hat[0, it] = npsum(p_scor_t * (z_vix[0, :it+1]) ** 2) / gamma_scor_t\n",
    "    sd_hat[0, it] = sqrt(mu2_hat[0, it] - (mu_hat[0, it]) ** 2)\n",
    "\n",
    "z_vix = (z_vix - mu_hat) / sd_hat\n",
    "VIXdate = VIX.Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersect the time series of invariants with the time series of the conditioning variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = np.in1d(VIXdate, dates[0])\n",
    "_, unique = np.unique(VIXdate[0, inter], return_index=True)\n",
    "indices = np.array(range(len(VIXdate[0])))[inter]\n",
    "tau_vix = indices[unique]\n",
    "inter = np.in1d(dates, VIXdate)\n",
    "_, unique = np.unique(dates[0, inter], return_index=True)\n",
    "indices = np.array(range(len(dates[0])))[inter]\n",
    "tau_epsi = indices[unique]\n",
    "\n",
    "z_vix = z_vix[0, tau_vix]\n",
    "epsi_c1 = epsi_c1[:, tau_epsi]\n",
    "epsi_c3 = epsi_c3[:, tau_epsi]\n",
    "i_c1, _ = epsi_c1.shape\n",
    "i_c3, t_ = epsi_c3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the state and time conditioning probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vix_star = z_vix[-1]  # target value\n",
    "prior = exp(-(log(2) / tauHL_prior) * abs(arange(t_, 1 + -1, -1))).reshape(1,-1)\n",
    "prior = prior / npsum(prior)\n",
    "\n",
    "# conditioner\n",
    "conditioner = namedtuple('conditioner', ['Series', 'TargetValue', 'Leeway'])\n",
    "conditioner.Series = z_vix.reshape(1, -1)\n",
    "conditioner.TargetValue = z_vix_star.reshape(1, -1)\n",
    "conditioner.Leeway = alpha\n",
    "\n",
    "p = ConditionalFP(conditioner, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the t copula of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate marginal distributions by fitting a Student t distribution via\n",
    "# MLFP and recover the invariants' grades\n",
    "\n",
    "# cluster 1\n",
    "u1 = zeros((i_c1, t_))\n",
    "nu_c1_marg = zeros(i_c1)\n",
    "mu_c1_marg = zeros(i_c1)\n",
    "sig2_c1_marg = zeros(i_c1)\n",
    "for i in range(i_c1):\n",
    "    mu_nu = zeros(nu_)\n",
    "    sig2_nu = zeros(nu_)\n",
    "    like_nu = zeros(nu_)\n",
    "    for k in range(nu_):\n",
    "        nu_k = nu_vec[k]\n",
    "        mu_nu[k], sig2_nu[k], _ = MaxLikelihoodFPLocDispT(epsi_c1[[i], :], p, nu_k, 10 ** -6, 1)\n",
    "        epsi_t = (epsi_c1[i, :] - mu_nu[k]) / sqrt(sig2_nu[k])\n",
    "        like_nu[k] = npsum(p * log(t.pdf(epsi_t, nu_k) / sqrt(sig2_nu[k])))  # likelihood\n",
    "        j_nu = argsort(like_nu)[::-1]\n",
    "\n",
    "    # take as estimates the parameters giving rise to the highest likelihood\n",
    "    nu_c1_marg[i] = max(nu_vec[j_nu[0]], 10)\n",
    "    mu_c1_marg[i] = mu_nu[j_nu[0]]\n",
    "    sig2_c1_marg[i] = sig2_nu[j_nu[0]]\n",
    "\n",
    "# cluster 3\n",
    "u3 = zeros((i_c3, t_))\n",
    "nu_c3_marg = zeros(i_c3)\n",
    "mu_c3_marg = zeros(i_c3)\n",
    "sig2_c3_marg = zeros(i_c3)\n",
    "for i in range(i_c3):\n",
    "    mu_nu = zeros(nu_)\n",
    "    sig2_nu = zeros(nu_)\n",
    "    like_nu = zeros(nu_)\n",
    "    for k in range(nu_):\n",
    "        nu_k = nu_vec[k]\n",
    "        idx = where(~np.isnan(epsi_c3[0]))[0][0]\n",
    "        p_k = p[0,idx:] / npsum(p[0,idx:])\n",
    "        mu_nu[k], sig2_nu[k], _ = MaxLikelihoodFPLocDispT(epsi_c3[[i], idx:], p_k, nu_k, 10 ** -6, 1)\n",
    "        epsi_t = (epsi_c3[i, idx:] - mu_nu[k]) / sqrt(sig2_nu[k])\n",
    "        like_nu[k] = npsum(p_k * log(t.pdf(epsi_t, nu_k) / sqrt(sig2_nu[k])))  # likelihood\n",
    "        j_nu = argsort(like_nu)[::-1]\n",
    "\n",
    "    # take as estimates the parameters giving rise to the highest likelihood\n",
    "    nu_c3_marg[i] = maximum(nu_vec[j_nu[0]], 10)\n",
    "    mu_c3_marg[i] = mu_nu[j_nu[0]]\n",
    "    sig2_c3_marg[i] = sig2_nu[j_nu[0]]\n",
    "\n",
    "# Map the grades into standard Student t realizations\n",
    "\n",
    "# cluster 1\n",
    "epsi_c1_tilde = zeros((i_c1, t_))\n",
    "for i in range(i_c1):\n",
    "    u1[i, :] = t.cdf((epsi_c1[i, :] - mu_c1_marg[i]) / sqrt(sig2_c1_marg[i]), nu_c1_marg[i])\n",
    "    epsi_c1_tilde[i, :] = t.ppf(u1[i, :], nu_c1)\n",
    "\n",
    "# cluster 3\n",
    "epsi_c3_tilde = zeros((i_c3, t_))\n",
    "for i in range(i_c3):\n",
    "    u3[i, :] = t.cdf((epsi_c3[i, :] - mu_c3_marg[i]) / sqrt(sig2_c3_marg[i]), nu_c3_marg[i])\n",
    "    epsi_c3_tilde[i, :] = t.ppf(u3[i, :], nu_c3)\n",
    "\n",
    "# fit the ellipsoid via MLFP\n",
    "\n",
    "# cluster 1\n",
    "_, sigma2,_ = MaxLikelihoodFPLocDispT(epsi_c1_tilde, p, nu_c1, 10 ** -6, 1)\n",
    "rho2_c1 = np.diagflat(diag(sigma2) ** (-1 / 2))@sigma2@np.diagflat(diag(sigma2) ** (-1 / 2))\n",
    "\n",
    "# cluster 3\n",
    "_, sigma2 = DiffLengthMLFP(epsi_c3_tilde, p, nu_c3, 10**-6)\n",
    "rho2_c3 = np.diagflat(diag(sigma2) ** (-1 / 2))@sigma2@np.diagflat(diag(sigma2) ** (-1 / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time series of the cluster 1 aggregating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tilde_c1 = zeros((i_c1, t_))\n",
    "# factor analysis\n",
    "rho2_c1_LRD, beta_c1, *_ = FactorAnalysis(rho2_c1, array([[0]]), k_c1)\n",
    "beta_c1 = np.real(beta_c1)\n",
    "\n",
    "# inverse LRD correlation\n",
    "delta2_c1 = diag(eye((i_c1)) - beta_c1@beta_c1.T)\n",
    "omega2_c1 = diagflat(1 / delta2_c1)\n",
    "rho2_c1_inv = omega2_c1 - (omega2_c1@beta_c1).dot(pinv((beta_c1.T@omega2_c1@beta_c1 + eye(k_c1))))@beta_c1.T@omega2_c1\n",
    "\n",
    "# time series aggregating variable\n",
    "z_tilde_c1 = beta_c1.T@rho2_c1_inv@epsi_c1_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the time series of the cluster 3 aggregating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig, _ = pcacov(rho2_c3)\n",
    "e = eig[:, :k_c3].T\n",
    "z_tilde_c3 = e@epsi_c3_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the MLFP estimate of the correlation matrix of the aggregating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_c1 = zeros(z_tilde_c1.shape)\n",
    "z_c3 = zeros(z_tilde_c3.shape)\n",
    "for i in range(k_c1):\n",
    "    z_c1[i, :] = t.ppf(t.cdf(z_tilde_c1[i, :], nu_c1), nu_aggr)\n",
    "\n",
    "for i in range(k_c3):\n",
    "    z_c3[i, :] = t.ppf(t.cdf(z_tilde_c3[i, :], nu_c3), nu_aggr)\n",
    "\n",
    "_,sig2_aggr = DiffLengthMLFP(r_[z_c1, z_c3], p, nu_aggr, 10**-6)\n",
    "rho2_aggr = np.diagflat(diag(sig2_aggr) ** (-1 / 2))@sig2_aggr@np.diagflat(diag(sig2_aggr) ** (-1 / 2))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
