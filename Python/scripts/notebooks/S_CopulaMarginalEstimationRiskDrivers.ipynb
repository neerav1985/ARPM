{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S_CopulaMarginalEstimationRiskDrivers [<img src=\"https://www.arpm.co/lab/icons/icon_permalink.png\" width=30 height=30 style=\"display: inline;\">](https://www.arpm.co/lab/redirect.php?code=S_CopulaMarginalEstimationRiskDrivers&codeLang=Python)\n",
    "For details, see [here](https://www.arpm.co/lab/redirect.php?permalink=ExerCopulaEstim2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import sys\n",
    "\n",
    "sys.path.append(path.abspath('../../functions-legacy'))\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from numpy import arange, array, ones, zeros, cumsum, percentile, diff, linspace, diag, eye, abs, log, exp, sqrt, tile, r_\n",
    "from numpy import sum as npsum\n",
    "from numpy.linalg import solve\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy.linalg import expm\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from CONFIG import GLOBAL_DB, TEMPORARY_DB\n",
    "from ARPM_utils import struct_to_dict, datenum\n",
    "from FPmeancov import FPmeancov\n",
    "from intersect_matlab import intersect\n",
    "from RollPrices2YieldToMat import RollPrices2YieldToMat\n",
    "from ConditionalFP import ConditionalFP\n",
    "from MaxLikelihoodFPLocDispT import MaxLikelihoodFPLocDispT\n",
    "from MMFP import MMFP\n",
    "from CopMargSep import CopMargSep\n",
    "from VGpdf import VGpdf\n",
    "from ParamChangeVG import ParamChangeVG\n",
    "from VAR1toMVOU import VAR1toMVOU\n",
    "from FitVAR1 import FitVAR1\n",
    "from ShiftedVGMoments import ShiftedVGMoments\n",
    "from FitCIR_FP import FitCIR_FP\n",
    "from InverseCallTransformation import InverseCallTransformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the databases and match the time series of interest to work with synchronous observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_Stocks'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_Stocks'), squeeze_me=True)\n",
    "\n",
    "SPX = struct_to_dict(db['SPX'],as_namedtuple=False)\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_SwapCurve'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_SwapCurve'), squeeze_me=True)\n",
    "\n",
    "DF_Rolling = struct_to_dict(db['DF_Rolling'],as_namedtuple=False)\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_OptionStrategy'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_OptionStrategy'), squeeze_me=True)\n",
    "\n",
    "OptionStrategy = struct_to_dict(db['OptionStrategy'],as_namedtuple=False)\n",
    "\n",
    "try:\n",
    "    db = loadmat(os.path.join(GLOBAL_DB, 'db_VIX'), squeeze_me=True)\n",
    "except FileNotFoundError:\n",
    "    db = loadmat(os.path.join(TEMPORARY_DB, 'db_VIX'), squeeze_me=True)\n",
    "\n",
    "VIX = struct_to_dict(db['VIX'],as_namedtuple=False)\n",
    "\n",
    "DateOptStrat = array([datenum(i) for i in OptionStrategy['Dates']]).T\n",
    "common, i_spvix, i_rates = intersect(SPX['Date'], DF_Rolling['Dates'])\n",
    "SPX['Date'] = SPX['Date'][i_spvix]\n",
    "SPX['Price_close'] = SPX['Price_close'][i_spvix]\n",
    "VIX['value'] = VIX['value'][i_spvix]\n",
    "VIX['Date'] = VIX['Date'][i_spvix]\n",
    "DF_Rolling['Dates'] = DF_Rolling['Dates'][i_rates]\n",
    "DF_Rolling['Prices'] = DF_Rolling['Prices'][:, i_rates]\n",
    "common, i_others, i_options = intersect(common, DateOptStrat)\n",
    "SPX['Date'] = SPX['Date'][i_others]\n",
    "SPX['Price_close'] = SPX['Price_close'][i_others]\n",
    "VIX['value'] = VIX['value'][i_others]\n",
    "VIX['Date'] = VIX['Date'][i_others]\n",
    "DF_Rolling['Dates'] = DF_Rolling['Dates'][i_others]\n",
    "DF_Rolling['Prices'] = DF_Rolling['Prices'][:, i_others]\n",
    "DateOptStrat = DateOptStrat[i_options]\n",
    "OptionStrategy['cumPL'] = OptionStrategy['cumPL'][i_options]\n",
    "t_common = len(common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the invariants from the Heston process as follows:\n",
    "## - Estimate the realized variance y_{t}  for the S&P500 using backward-forward exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_HST = log(SPX['Price_close']).reshape(1,-1)\n",
    "dx_HST = diff(x_HST)\n",
    "\n",
    "# Estimate the realized variance\n",
    "s_ = 252  # forward/backward parameter\n",
    "lambda1_HST = log(2) / (2*252)  # half life 2 years\n",
    "p_y = exp(-lambda1_HST*abs(arange(-s_,s_+1))).reshape(1,-1)\n",
    "p_y = p_y / npsum(p_y)\n",
    "\n",
    "t_ = dx_HST.shape[1] - 2*s_\n",
    "y = zeros(t_)\n",
    "for s in range(t_):\n",
    "    dx_temp = dx_HST[[0],s:s + 2*s_+1]\n",
    "    y[s] = p_y@(dx_temp.T** 2)\n",
    "# daily variance\n",
    "\n",
    "dy = diff(y)\n",
    "dx_HST = dx_HST[[0],s_:-s_]\n",
    "x_HST = x_HST[[0],s_:-s_]\n",
    "\n",
    "# - Fit the CIR process to by FitCIR_FP\n",
    "t_obs = len(dy)  # time series len\n",
    "p_HST = ones((1, t_obs)) / t_obs  # flexible probabilities\n",
    "delta_t = 1  # fix the unit time-step to 1 day\n",
    "par_CIR = FitCIR_FP(y[1:], delta_t, None, p_HST)\n",
    "kappa = par_CIR[0]\n",
    "y_bar = par_CIR[1]\n",
    "eta = par_CIR[2]\n",
    "\n",
    "# - Estimate the drift parameter and the correlation coefficient between the Brownian motions by FPmeancov\n",
    "mu_HST, sigma2_x_HST = FPmeancov(r_[dx_HST[[0],1:], dy.reshape(1,-1)], p_HST)  # daily mean vector and covariance matrix\n",
    "mu_x_HST = mu_HST[0]  # daily mean\n",
    "rho_HST = sigma2_x_HST[0, 1] / sqrt(sigma2_x_HST[0, 0]*sigma2_x_HST[1,1])  # correlation parameter\n",
    "\n",
    "# - Extract the invariants\n",
    "epsi_x_HST = (dx_HST[[0],1:] - mu_x_HST*delta_t) / sqrt(y[1:])\n",
    "epsi_y = (dy + kappa*(y[1:]-y_bar)*delta_t) / (eta*sqrt(y[1:]))\n",
    "epsi_HST = r_[epsi_x_HST, epsi_y.reshape(1,-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the invariants for the MVOU process as follows:\n",
    "## - Compute the 2-year and 7-year yield to maturity by RollPrices2YieldToMat and obtain the corresponding shadow rates by InverseCallTransformation\n",
    "## Select the two-year and seven-year key rates and estimate the MVOU process\n",
    "## parameters using functions FitVAR1 and VAR1toMVOU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, pick = intersect([2, 7], DF_Rolling['TimeToMat']) # select rates (.T2y','7y')\n",
    "yields,_ = RollPrices2YieldToMat(DF_Rolling['TimeToMat'][pick], DF_Rolling['Prices'][pick,:])\n",
    "yields = yields[:, s_ + 1:-s_]\n",
    "d_ = len(pick)\n",
    "\n",
    "# - Fit the parameters by FitVAR1 and VAR1toMVOU and extract the invariants\n",
    "eta_ICT = 0.013\n",
    "x_MVOU = InverseCallTransformation(yields, {1:eta_ICT})  # select rates ('2y','7y')\n",
    "lam = log(2) / (21*9)  # half-life: 9 months\n",
    "p_MVOU = exp(-lam*arange(t_obs, 1 + -1, -1)).reshape(1,-1)\n",
    "p_MVOU = p_MVOU / npsum(p_MVOU)\n",
    "[alpha, b, sig2_U] = FitVAR1(x_MVOU, p_MVOU, 4)\n",
    "    # [alpha, b, sig2_U] = FitVAR1(dx_MVOU, x_MVOU(:, 1:-1),p_MVOU, 4)\n",
    "mu_MVOU, theta_MVOU, sigma2_MVOU,_ = VAR1toMVOU(alpha, b, sig2_U, delta_t)\n",
    "# [mu_MVOU, theta_MVOU, sigma2_MVOU] = FitVAR1MVOU(dx_MVOU, x_MVOU(:, 1:-1), delta_t, p_MVOU, 4)\n",
    "epsi_MVOU = x_MVOU[:, 1:] - expm(-theta_MVOU*delta_t)@x_MVOU[:, : -1] + tile((eye(theta_MVOU.shape[0]) - expm(-theta_MVOU*delta_t))@solve(theta_MVOU,mu_MVOU)[...,np.newaxis], (1, t_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the invariants for the variance gamma process and fit their parametric distribution as follows\n",
    "## - Compute the time series of daily P&L and extract the invariants\n",
    "## cumulative P&L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_VG = OptionStrategy['cumPL'][s_+1:-s_].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## invariants (VG is a Levy process-> random walk -> the invariants are the increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsi_VG = diff(x_VG)\n",
    "\n",
    "# -Fit the parameters of the VG marginal distribution by MMFP\n",
    "# initial guess on parameters\n",
    "mu0 = 1\n",
    "theta0 = -1\n",
    "sigma0 = 1\n",
    "nu0 = 1\n",
    "par0 = [mu0, theta0, sigma0, nu0]\n",
    "\n",
    "flat_p = ones((1, t_obs)) / t_obs  # flat flexible probabilities\n",
    "\n",
    "HFP = namedtuple('HFP', ['FlexProbs','Scenarios'])\n",
    "HFP.FlexProbs = flat_p\n",
    "HFP.Scenarios = epsi_VG\n",
    "par = MMFP(HFP, 'SVG', par0)\n",
    "\n",
    "mu_vg = par.c\n",
    "theta_vg = par.theta\n",
    "sigma_vg = par.sigma\n",
    "nu_vg = par.nu\n",
    "\n",
    "# -After switching to the (c,m,g) parametrization, compute the marginal pdf and recover the cdf numerically\n",
    "[par.c, par.m, par.g] = ParamChangeVG(theta_vg, sigma_vg, nu_vg)  # change the parametrization to compute the pdf\n",
    "\n",
    "# compute the expectation and variance to fix the grid for the pdf\n",
    "expectation_vg, variance_vg, _, _ = ShiftedVGMoments(0, theta_vg, sigma_vg, nu_vg, 1)\n",
    "epsi_grid_vg = linspace(expectation_vg - 4*sqrt(variance_vg), expectation_vg + 4*sqrt(variance_vg), t_obs).reshape(1,-1)\n",
    "\n",
    "pdf_vg = VGpdf(epsi_grid_vg, par, 1)\n",
    "cdf_vg = cumsum(pdf_vg / npsum(pdf_vg),axis=1)\n",
    "shifted_epsi_grid_vg = epsi_grid_vg + mu_vg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Flexible Probabilities (conditioned on VIX via Entropy Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioner\n",
    "conditioner = namedtuple('conditioner', ['Series', 'TargetValue', 'Leeway'])\n",
    "conditioner.Series = VIX['value'][- t_obs:].reshape(1,-1)\n",
    "conditioner.TargetValue = np.atleast_2d(VIX['value'][-1])\n",
    "conditioner.Leeway = 0.35\n",
    "# prior\n",
    "lam = log(2) / (5*252)  # half life 5y\n",
    "prior = exp(-lam*abs(arange(t_obs, 1 + -1, -1)))\n",
    "prior = prior / npsum(prior)\n",
    "\n",
    "p = ConditionalFP(conditioner, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the extracted invariants in a matrix and fit the copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invariants\n",
    "epsi = r_[epsi_HST, epsi_MVOU, epsi_VG]\n",
    "i_ = epsi.shape[0]\n",
    "t_obs = epsi.shape[1]\n",
    "\n",
    "# Rescale the invariants\n",
    "q1 = percentile(epsi, 25, axis=1,keepdims=True)\n",
    "q2 = percentile(epsi, 75, axis=1,keepdims=True)\n",
    "interq_range = q2 - q1\n",
    "epsi_rescaled = epsi / tile(interq_range, (1, t_obs))\n",
    "\n",
    "# STEP 1: Invariants grades\n",
    "epsi_grid, u_grid, grades = CopMargSep(epsi_rescaled, p)\n",
    "nu = 4\n",
    "\n",
    "# STEP [1:] Marginal t\n",
    "epsi_st = zeros(epsi.shape)\n",
    "for i in range(i_):\n",
    "    epsi_st[i,:] = t.ppf(grades[i,:], nu)\n",
    "\n",
    "# STEP 3: Fit ellipsoid (MLFP ellipsoid under Student t assumption)\n",
    "Tol = 10 ** -6\n",
    "mu_epsi, sigma2_epsi,_ = MaxLikelihoodFPLocDispT(epsi_st, p, nu, Tol, 1)\n",
    "\n",
    "# STEP 4: Shrinkage (we don't shrink sigma2)\n",
    "\n",
    "# STEP 5: Correlation\n",
    "c2_hat = np.diagflat(diag(sigma2_epsi) ** (-1 / 2))@sigma2_epsi@np.diagflat(diag(sigma2_epsi) ** (-1 / 2))\n",
    "\n",
    "# Rescale back the invariants'o the original size\n",
    "epsi_grid = epsi_grid * tile(interq_range, (1, t_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal distributions: HFP distributions for epsi_HST and epsi_MVOU parametric VG distribution for epsi_VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginals_grid = r_[epsi_grid[:4,:], shifted_epsi_grid_vg.reshape(1,-1)]\n",
    "marginals_cdfs = r_[u_grid[:4,:], cdf_vg]\n",
    "\n",
    "varnames_to_save = ['d_','marginals_grid','marginals_cdfs','c2_hat','mu_epsi','nu','eta_ICT','x_MVOU','mu_MVOU','theta_MVOU','sigma2_MVOU',\n",
    "                    'delta_t','kappa','y_bar','eta','mu_x_HST','x_HST','mu_vg','theta_vg','sigma_vg','nu_vg','x_VG','y']\n",
    "vars_to_save = {varname: var for varname, var in locals().items() if isinstance(var,(np.ndarray,np.float,np.int))}\n",
    "vars_to_save = {varname:var for varname,var in vars_to_save.items() if varname in varnames_to_save}\n",
    "savemat(os.path.join(TEMPORARY_DB, 'db_CopulaMarginalRiskDrivers'),vars_to_save)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
